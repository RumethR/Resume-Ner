{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Kg-JhFakYlkM",
        "OWvidz0phH8z",
        "mfdKFI73e88o"
      ],
      "authorship_tag": "ABX9TyNMQ7zpU4uyDShoHqxbreU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RumethR/Resume-Ner/blob/main/Cw1_w1809911_RumethRandombage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Application Area Review\n",
        "\n",
        "**Selected Domain: Human resources and computing.**\n"
      ],
      "metadata": {
        "id": "Kg-JhFakYlkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The domain of human resources within an organisation entails a wide variety of operations and responsibilities ranging from employee compensation to organisational structure. This poses a great opportunity for Artificial Intelligent (AI) systems to contribute towards the Human Resource Management (HRM) tasks within an organisation. One of the most common areas where AI has been highly used is within the realm of recruitment and talent acquisition. A study by the ‘Oracle’ corporation has revealed that professionals in the human resource management industry believe that AI can boost productivity among other statistics (Rahmani and Kamberaj, 2021). Other sources believe that AI will enable new application procedures for candidates, increase applicant discovery rates and even reduce vacancy times as these systems can traverse through applications faster than trained professionals (Vrontis et al., 2021). AI powered applications in HRM are also able to process large amounts of data at high speeds, faster and more accurately than any trained professional, this allows organisations to gain insights that would otherwise be impossible with manual labour. The use of these systems are also more likely to be less biassed towards factors that do not affect performance such as age, gender or race (assuming that they are developed properly). Repetitive tasks within the domain of HRM are more likely to be successfully automated with AI powered applications according to *Vrontis et al. (2021)*.\n",
        "\n",
        "The overall effect of bias that is induced when training was studied by the influential work of *Tambe, Cappelli and Yakubovich (2019)* who observed that the data that these systems were built on had a major impact in how these systems performed. This underlying data itself has the opportunity to represent industry swings and unspoken ethics (i.e. the system might be trained on data that indicated towards hiring from a specific university), hence it is important that these systems work in tandem with trained professionals.This study also looks into how Machine Learning algorithms have fared against several different HR operations within organisations. AI powered HRM task management systems such as Quine, BenefitFocus and Jobvite are now widely available for any organisation of any scale to incorporate into their workflows (Tambe, Cappelli and Yakubovich, 2019).\n",
        "\n",
        "On the opposite side of the spectrum, employees now have access to tools such as IBM’s ‘Blue Match’ presents viable job openings for their specific skill set, similarly AI powered tools have helped IBM to identify possible flight risks and retain employees for much longer according to their former CEO Ginni Rometty (Rosenbaum, 2019). These HRM service vendors however do not provide any information about the type of AI techniques that are used internally within their products. It is also important to note that most AI powered solutions can typically be applied on a small portion of a HRM task, for example in the task of vetting candidates these tools can only provide a suggestion or score of desirability is most cases rather than performing the entire stack of recruiting tasks that go alongside the vetting process (Ghosh, Majumder and Santosh Kumar Das, 2023). The common theme among most studies that observe the usage of AI in HRM is that they are heavily reliant on the quality and the quantity of the data that they are being built upon.\n",
        "\n",
        "With the rise of open source Large Language Models (LLM) it is easier than ever for organisations to create a custom tailored algorithm that can perform trivial tasks within HRM such as answering frequently asked questions from employees through chatbots *Pawan Budhwar et al. (2023)* studied the use of popular generative AI tool ChatGPT in a HRM context and how it affects areas other than productivity such as employment relations, employee wellbeing and engagement. This study revealed that employees prefer tools such as ChatGPT in many scenarios that require understanding and perceiving information more than their human co-workers. However long term social, ethical and legal implications of using such technologies are difficult to quantify (Pawan Budhwar et al., 2023). Listed below are some other AI-based techniques used in the HRM context (Ghosh, Majumder and Santosh Kumar Das, 2023).\n",
        "\n",
        "*   Turnover Prediction with Artificial Neural Networks\n",
        "*   Candidate Search With Knowledge-Based Search Engines\n",
        "*   Sentiment Analysis on employee engagement and relations\n",
        "*   Résumé data acquisition using NLP\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PdkdKZArfPO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B: Comparision and evaluation of AI Techniques used in Human Resources"
      ],
      "metadata": {
        "id": "OWvidz0phH8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned before all HRM tasks in an organization cannot be supplemented or automated by one singular system. The scope of this specific section will cover the task of utilizing Machine Learning techniques for the purpose of recruiting, vetting and staffing employees using a CV/Résumé. This project will focus on using Named Entity Recognition (NER) on CVs to identify talent that best suites a given job posting. NER is classified as a Sequence Labelling Task within the domain of Natural Language Processing (NLP), there are many different ways of addressing these sequence labelling tasks listed below are three such approaches."
      ],
      "metadata": {
        "id": "itzimModhSzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using Conditional Random Fields (CRF)"
      ],
      "metadata": {
        "id": "m5YLiQ2f9-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Conditional Random Fields for NER tasks is a popular machine learning based approach which uses a probabilistic model. CRFs capture dependencies among neighbouring labels in a sequence and have the ability to consider the entire input sequence when making predictions. Feature functions are used to define relationships between input features (words) and output labels (entities). These relationships have to be manually defined by the trainer. CRFs calculate the probability distribution of an output sequence (labels) given an input sequence.Transition weights in Conditional Random Fields represent the likelihood of moving from one label to another in the sequence. During the training phase, the relationships between input features, their corresponding output labels (using data from the feature functions) and the effect of the transition weights are learned from the training data.\n",
        "\n",
        "For training a model that can perform NER using CRF text data with entities as labels are required. Necessary words in the training data sequence need to be accompanied by its corresponding entity label. Datasets for training NER models are widely available under different platforms such as Kaggle and the UCI Machine Learning Repository. One of the most common dataset and evaluation benchmark for NER is the CoNLL-2003 [dataset](https://www.clips.uantwerpen.be/conll2003/ner/). CRF based models tend to perform well as they have the ability to capture contextual information about the data. However performance is heavily dependent on the quality and relevance of engineered features, this is one of the major drawbacks of CRF based models. Feature engineering will mostly depend on the domain that the information is extracted from, and in the context of CVs people vary in the way they express information hence engineering features that capture necessary information will be quite a difficult task. Generally, CRF based models tend to be computationally efficient compared to their Deep Learning based counterparts who in turn will be better at capturing more intricate patterns within the data."
      ],
      "metadata": {
        "id": "YT0ru1sb-qMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using Neural Networks and pre-trained models"
      ],
      "metadata": {
        "id": "uQRcMfG2-IX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently there are many frameworks and pre-trained models that ease the task of NER. Similar to Tensorflow there are libraries that are built for performing NLP tasks. [SpaCy](https://spacy.io/usage/spacy-101) is one such Python package that offers pre-trained models for tasks such as NER. SpaCy’s default models use Convolutional Neural Networks (CNN) for the task of feature extraction and entity recognition. These models are trained on large labelled datasets hence they cover a wide range of contexts and features. By default the SpaCy package can identify entities within a  given text such as locations, dates, persons and organisations. These can be customised through a variety of API’s that are also included within the library. Similar to CRF and Transformer based models SpaCy’s CNNs can also understand contextual information regarding the input data, allowing the model to capture relationships between entities.\n",
        "\n",
        "Efficiency of a custom built CRF based model for NER depends heavily on the training data and the quality of the engineered features. Comparatively pre-trained models from SpaCy can offer similar accuracy scores and be more computationally efficient at  the same time. However by using a pre-built model the level of customization is limited. There are other libraries that have packaged in pre-trained models for NER such as [NLTK](https://www.nltk.org/) (Natural Language Toolkit) which can offer similar functionality to SpaCy, each library has their own strengths and weaknesses."
      ],
      "metadata": {
        "id": "5LRr3UGx_Jat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using pre-trained Transformer based models (BERT)"
      ],
      "metadata": {
        "id": "E399fVqg-Mmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the rise of pre-trained Transformer based models such as BERT and GPT, the application of these models for NLP tasks has proven to yield competitive results. The Bi-directional approach considers text preceding from both the left and the right of a given word in a sentence. This helps the model to understand a much broader context (and longer text sequences) compared to the default pre-trained models offered by SpaCy which uses a CNN. This can be especially useful in the context of parsing data from CVs which can contain ambiguous and complex sentences. There are many more pre-trained Transformer based models which can have this characteristic, these are available through the [Hugging Face](https://huggingface.co/) community platform. BERT specifically is trained on a large corpus of text using 2 unsupervised learning techniques, masked language modelling and next sentence prediction.\n",
        "\n",
        "Unlike SpaCy, BERT is not an out-of-the-box solution for most NER tasks. This model needs to be fine-tuned in order to be used for NER purposes. This allows BERT to be domain specific and can in theory lead to better performance on very complex and domain specific NER tasks. The drawback of this level of customization is that a BERT model is more resource intensive and technically challenging to develop compared to SpaCy or NLTK. It also requires a labelled dataset for training before it can be used. Fine-tuning a BERT model requires large amounts of data to yield competitive results, which might not be publicly available in the case of CVs. In summary, BERT models can perform better than the other two approaches given that they are fine-tuned correctly, whilst SpaCy offers a better efficiency to performance ratio without requiring extensive training or customization. Although compared to training a CRF based model for NER form scratch customising a BERT model will yield better results, one drawback is it being more computationally intensive to train and use.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nf2NHgPdLZsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C: Implementation"
      ],
      "metadata": {
        "id": "bbGLUtHfjtY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using a text-based resume dataset available from Kaggle, the dataset will be accessed directly via this notebook using the Kaggle API. The credentials are hardcoded onto this notebook for ease-of-use during the viva. (Not Recommended)"
      ],
      "metadata": {
        "id": "HQY7HRr3tl_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"rumethrandombage\",\"key\":\"6c49cc180f0498fccf2dabb3d22f0236\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "import json\n",
        "\n",
        "!kaggle datasets download -d dataturks/resume-entities-for-ner\n",
        "!unzip resume-entities-for-ner.zip #Extracts to ./content\n",
        "\n",
        "dataset = [] # Each element contains a json object\n",
        "with open('/content/Entity Recognition in Resumes.json', 'r') as file:\n",
        "    for line in file:\n",
        "        dataset.append(json.loads(line))\n",
        "\n",
        "print(\"Number of elements in dataset list\", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I-uBUKswCVZ",
        "outputId": "1c43409c-9110-4228-ec65-3d55cb826322"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Downloading resume-entities-for-ner.zip to /content\n",
            "  0% 0.00/323k [00:00<?, ?B/s]\n",
            "100% 323k/323k [00:00<00:00, 14.6MB/s]\n",
            "Archive:  resume-entities-for-ner.zip\n",
            "  inflating: Entity Recognition in Resumes.json  \n",
            "Number of elements in dataset list 220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 220 JSON objects. The structure of each JSON object is as follows:\n",
        "\n",
        "> \"content\" -> (contains all the data in the CV as a string)\n",
        "\n",
        "> \"annotations\" -> annotation label (\"Name\", \"Skills\", etc.) -> points (\"start and stop index where the label is contained\") - > text for label\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HpyO5Zi-CGmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy is packaged with a CNN based pre-built model that can perform NER for general entity types (Persons, Organizations, Locations, etc.)."
      ],
      "metadata": {
        "id": "XftpgsXqj1BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdJX7D5damP1",
        "outputId": "6765a6cc-9c9b-4e29-d829-1a150c7b2206"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 12:33:53.631246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 12:33:53.631315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 12:33:53.632911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 12:33:53.641642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 12:33:55.000448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Access one string of text without any annotations from the datset and pass it into the pre-trained spacy model\n",
        "first_entry = dataset[0]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(first_entry[\"content\"])\n",
        "\n",
        "# Let's see how the pre-trained Model performed\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCYOyakvCGJN",
        "outputId": "adcbdef6-43c3-44d1-cd6b-a24d2ae9df98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abhishek Jha\n",
            "Application PERSON\n",
            "Bengaluru GPE\n",
            "Karnataka - Email PERSON\n",
            "Karnataka GPE\n",
            "November 2017 DATE\n",
            "B.E in Information ORG\n",
            "B.v.b GPE\n",
            "Hubli ORG\n",
            "Karnataka GPE\n",
            "August 2013 to June 2017\n",
            "\n",
            "12th DATE\n",
            "Mathematics NORP\n",
            "April 2011 to March 2013 DATE\n",
            "10th ORDINAL\n",
            "Kendriya Vidyalaya PERSON\n",
            "April 2001 to March 2011 DATE\n",
            "Less than 1 year DATE\n",
            "Less than 1 year DATE\n",
            "Database Management ORG\n",
            "Less than 1 year DATE\n",
            "Database Management System ORG\n",
            "Less than 1 year DATE\n",
            "Java PERSON\n",
            "Less than 1 year DATE\n",
            "• Programming PRODUCT\n",
            "C++ PERSON\n",
            "Java\n",
            "• Oracle PERSON\n",
            "• Computer Networks ORG\n",
            "Linux GPE\n",
            "Windows GPE\n",
            "Non - Technical Skills\n",
            "\n",
            "• Honest ORG\n",
            "Calm\n",
            "• Team-Player PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The built-in NER model performs extremely poorly without any customisations. We can use transfer learning or we can create our own model using the NER pipeline provided by SpaCy, we will be creating a custom model for this project."
      ],
      "metadata": {
        "id": "HVhLRec3VUk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm # Used to show progress bars\n",
        "from spacy.util import filter_spans # To make sure there are no overlapping entities\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "labels = [\"Name\", \"College Name\", \"Degree\", \"Graduation Year\", \"Years of Experience\", \"Companies worked at\", \"Designation\", \"Skills\", \"Location\", \"Email Address\"]\n",
        "\n",
        "#ner = nlp.add_pipe(\"ner\") #only using the ner component for the pipeline provided by SpaCy\n",
        "ner = nlp.get_pipe('ner')\n",
        "\n",
        "# Add the pre-defined labels gathered from the dataset to the pipeline\n",
        "for label in labels:\n",
        "  ner.add_label(label)"
      ],
      "metadata": {
        "id": "oEso7kj8tZx7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required format for SpaCy to train model.\n",
        "\n",
        "{\n",
        "    \"text\": \"Apple Inc. is headquartered in California.\",\n",
        "    \"entities\": [\n",
        "        {\"start\": 0, \"end\": 10, \"label\": \"ORG\"},\n",
        "        {\"start\": 36, \"end\": 46, \"label\": \"GPE\"}\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "5OAbpP-dpxnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current format (After extracting from dataset)\n",
        "\n",
        "{\n",
        "  \"content\": \"Rumeth Randombage is a student at IIT, with one year Internship experience. Email .....\",\n",
        "\n",
        "  annotation: [\n",
        "    { [label: \"Skills\"],  \n",
        "      [points:\n",
        "      {start: 1295,\n",
        "      end: 1621,\n",
        "      text: Python}\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "A3gPGSPAp8RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# Read the dataset and pre-process as needed for SpaCy\n",
        "print(\"Example before preprocessing data\")\n",
        "print(dataset[0])\n",
        "\n",
        "# Convert the data to spaCy's training data format\n",
        "formatted_data = []\n",
        "\n",
        "#First we are going to convert one record in the dataset and see if it meets the required format.\n",
        "first_entry = dataset[0] #Each entry contains a 'content', 'annotation' and 'extras'\n",
        "\n",
        "def process_dataset_entry(first_entry):\n",
        "  raw_text = first_entry[\"content\"]\n",
        "\n",
        "  # Clean text: remove punctuation and newline characters\n",
        "  cleaned_text = re.sub(r'[\\n\\r]', ' ', raw_text)\n",
        "  cleaned_text = cleaned_text.replace(\"'\", \"\")\n",
        "\n",
        "  #cleaned_text = raw_text.translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ')\n",
        "\n",
        "  # Process cleaned text with spaCy\n",
        "  doc = nlp.make_doc(cleaned_text)\n",
        "\n",
        "  # Retrieve cleaned text without punctuation and newline characters\n",
        "  cleaned_text = ' '.join(token.text for token in doc if not token.is_punct and not token.is_space)\n",
        "\n",
        "  formatted_entry = {\"text\": \"\", \"entities\": []}\n",
        "  formatted_entry[\"text\"] = cleaned_text\n",
        "\n",
        "  # Make sure that the given start and end points for specific label lines up with the text\n",
        "  entities = []\n",
        "\n",
        "  # This loop checks if the labels and annotations are correct.\n",
        "  for i in first_entry[\"annotation\"]:\n",
        "    try:\n",
        "      label = i['label'][0]\n",
        "      annotation_text = i[\"points\"][0][\"text\"]\n",
        "\n",
        "      #clean the annotation text as before, and get the start and end points within the larger text\n",
        "      annotation_text_clean = re.sub(r'[\\n\\r]', ' ', annotation_text)\n",
        "      annotation_text_clean = annotation_text_clean.replace(\"'\", \"\")\n",
        "\n",
        "      # Process cleaned text with spaCy\n",
        "      doc = nlp.make_doc(annotation_text_clean)\n",
        "\n",
        "      # Retrieve cleaned text without punctuation and newline characters\n",
        "      cleaned_annotation_text = ' '.join(token.text for token in doc if not token.is_punct and not token.is_space)\n",
        "\n",
        "      start_index = cleaned_text.find(cleaned_annotation_text)\n",
        "      end_index = start_index + len(cleaned_annotation_text) - 1 if start_index != -1 else -1\n",
        "      if label in labels: # In case there are some labels that are not needed/wrong\n",
        "        entities.append({\"start\": start_index, \"end\": end_index, \"label\": label})\n",
        "        formatted_entry[\"entities\"] = entities\n",
        "    except IndexError:\n",
        "      print(\"There was an error parsing the label\")\n",
        "      print(first_entry)\n",
        "      return\n",
        "\n",
        "  discard_entries = 0 # Discarded because of overlapping\n",
        "  checked_entry = prevent_overlapping(formatted_entry)\n",
        "  if len(checked_entry[\"entities\"]) == 0:\n",
        "    print(\"There was overlapping within all the entities\")\n",
        "    discard_entries = discard_entries + 1\n",
        "    return\n",
        "  else:\n",
        "    formatted_data.append(formatted_entry)\n",
        "\n",
        "\n",
        "# Make sure the entities do not overlap within the same text span\n",
        "def prevent_overlapping(entry):\n",
        "  # Overlapping refers to when two labels point to the same text span\n",
        "  entities = entry[\"entities\"]\n",
        "\n",
        "  # Sort entities by start index to check for overlaps\n",
        "  sorted_entities = sorted(entities, key=lambda x: x['start'])\\\n",
        "\n",
        "  previous_end = -1\n",
        "  non_overlapping_entities = []\n",
        "  overlapping_entities = []\n",
        "  for entity in sorted_entities:\n",
        "    if entity['start'] >= previous_end:\n",
        "        non_overlapping_entities.append(entity)\n",
        "        previous_end = entity['end']\n",
        "    else:\n",
        "      overlapping_entities.append(entry)\n",
        "      #print(\"Overlapping Detected on the following entity\")\n",
        "      #print(entity)\n",
        "\n",
        "  # Create and return a new entry with the only the non overlapping entities\n",
        "  if len(non_overlapping_entities) >= 1:\n",
        "    entry[\"entities\"] = non_overlapping_entities\n",
        "    return entry\n",
        "\n",
        "for entry in dataset:\n",
        "  process_dataset_entry(entry)\n",
        "\n",
        "#print(formatted_data[0])\n",
        "print(len(formatted_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiUV0RRdnTHS",
        "outputId": "3ec376a2-f9cb-45eb-f174-2e4089659377"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example before preprocessing data\n",
            "{'content': \"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\", 'annotation': [{'label': ['Skills'], 'points': [{'start': 1295, 'end': 1621, 'text': '\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player'}]}, {'label': ['Skills'], 'points': [{'start': 993, 'end': 1153, 'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]}, {'label': ['College Name'], 'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]}, {'label': ['College Name'], 'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]}, {'label': ['Graduation Year'], 'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]}, {'label': ['College Name'], 'points': [{'start': 771, 'end': 813, 'text': 'B.v.b college of engineering and technology'}]}, {'label': ['Designation'], 'points': [{'start': 727, 'end': 769, 'text': 'B.E in Information science and engineering\\n'}]}, {'label': ['Companies worked at'], 'points': [{'start': 407, 'end': 415, 'text': 'Accenture'}]}, {'label': ['Designation'], 'points': [{'start': 372, 'end': 404, 'text': 'Application Development Associate'}]}, {'label': ['Email Address'], 'points': [{'start': 95, 'end': 145, 'text': 'Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n'}]}, {'label': ['Location'], 'points': [{'start': 60, 'end': 68, 'text': 'Bengaluru'}]}, {'label': ['Companies worked at'], 'points': [{'start': 49, 'end': 57, 'text': 'Accenture'}]}, {'label': ['Designation'], 'points': [{'start': 13, 'end': 45, 'text': 'Application Development Associate'}]}, {'label': ['Name'], 'points': [{'start': 0, 'end': 11, 'text': 'Abhishek Jha'}]}], 'extras': None}\n",
            "There was an error parsing the label\n",
            "{'content': \"Shrishti Chauhan\\nHave total work experience of 2.5 years on Oracle Fusion Middleware -\\nSOA, WebLogic and MFT Module.\\n\\nBilaspur, Chhattisgarh - Email me on Indeed: indeed.com/r/Shrishti-\\nChauhan/89d7feb4b3957524\\n\\nSeeking to hone and enhance my technical skills in Oracle Fusion Middleware while working as\\na professional in challenging and goal oriented environment.\\n\\nWilling to relocate to: Bengaluru, Karnataka\\n\\nWORK EXPERIENCE\\n\\nTechnical Consultant\\n\\nOracle -  Bengaluru, Karnataka -\\n\\nOctober 2015 to Present\\n\\n• Have total work experience of 2.5 years on Oracle Fusion Middleware - SOA, WebLogic and\\nMFT Module.\\n\\n• Have extensively worked on Support, Testing, Cloning, Monitoring and Maintenance support\\nand Enhancement for the E-Commerce Project with multi system module.\\n\\n• Have good understanding on End-to-End Business Process.\\n\\n• Experience in developing and deploying BPEL Processes using technology adapters (DB\\nAdapter, File Adapter, FTP Adapter and JMS Adapter).\\n\\n• Part of a team for developing a BPEL process to integrate Oracle Fusion Applications. This\\nOrchestrated BPEL Process had process activities like data conversion, transformation and fault\\nhandling.\\n\\n• Developed and deployed BPEL processes to import sales order and add lines to the sales order\\nimported from external sources (Files and Databases) into Order Orchestration Module.\\n\\n• Worked on BPEL process to create sales order within Order Orchestration Module using SOAPUI\\nand Enterprise Manage.\\n\\n• Have good understanding on Synchronous and Asynchronous processes, Transformations, XSD,\\nXSLT and XPath\\n\\n• Installation of Middle tier application server (SOA Suite 11g), configuring and deploying\\napplication adapters and integrating with other ERP or 3rd Party Services.\\n\\n• Working with the client team and communicating with the business teams regarding integration\\nwith external Delivery System.\\n\\nhttps://www.indeed.com/r/Shrishti-Chauhan/89d7feb4b3957524?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Shrishti-Chauhan/89d7feb4b3957524?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• As an Oracle Technical Consultant I was responsible for providing End-to-End support in the\\nproject for Oracle Fusion Middleware.\\n\\n• Good understanding of Service Oriented Architecture (SOA) with the middleware technologies\\nfor application integration.\\n\\nTechnical Consultant\\n\\nOracle\\n\\nHave total work experience of 2.5 years on Oracle Fusion Middleware - SOA, WebLogic and MFT\\nModule.\\n\\nEDUCATION\\n\\nC.S.\\n\\nCHHATTISGARH SWAMI VIVEKANANDA TECHNICAL UNIVERSITY\\n\\n2011 to 2015\\n\\nSKILLS\\n\\nXml, Oracle MFT, Core Java, Oracle SOA, WSDL, ODI\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILL SET\\nERP Packages Oracle Fusion Middleware and Oracle Fusion Application (DOO/GOP)\\nFusion Middleware Modules Oracle SOA, MFT and Web Logic (11G and 12C Version)\\nAdapters Database, JMS, FTP and File\\nTools JDeveloper, Enterprise Manager, Administrative Console, SOAP UI and BI Publisher\\nProgramming Languages XML, C++ and Core Java\\nDatabases SQL\\nOperating System Unix and Windows\\n\\nPROJECT DETAILS\\nProject Client HOLTS RENFREW\\nRole & Duration Technical Consultant Oct-15\\nScope of Project Oracle Fusion Middleware 11G and Oracle Fusion Application R12\\nProject Description\\nHolt Renfrew is 180 years old chain of high-end Canadian department stores specializing in an\\narray of luxury brands and designer boutiques.\\n\\nResponsibilities\\n• Worked as Technical Consultant to the customer for the issues related to Oracle SOA (mainly\\nBPEL and Mediator Components).\\n• Providing daily Server Health Check for SOA and DOO/GOP (SCM) to the customer.\\n• Attending/Conducting weekly customer calls and providing an update about the progress of\\nthe issues.\\n\\n\\n\\n• Interacting with the Clients to understand the requirement and details of the issues and\\nproviding suggestions/answers to their queries.\\n• Resolving technical issues and enhancement raised by the customer.\\n• Reviewing work products from the Team Members and delivering products to the client on time\\nwith high quality.\\n• RCA for the Complex Issues in client's version of Oracle SOA and SCM.\\n• Partially worked on functional issues related to DOO/GOP - Order Management and Inventory\\nManagement.\\n• Working effectively with Oracle Product Support for any issue related to Standard Functionality.\\n\\nProject Client WHITBREAD PLC\\nRole & Duration Technical Consultant SEPT-17\\nScope of Project Oracle Fusion Middleware 12C and Oracle SAAS\\nProject Description\\nWhitbread PLC is the UK's largest hospitality company, owning Premier Inn and Costa Coffee, as\\nwell as Beefeater, Brewers Fayre, and Bar Block.\\n\\nResponsibilities\\n• Worked as Technical Consultant to the customer for the issues related to Oracle SOA and MFT.\\n• Monitoring Critical Process and fixing the concurrent process running on SOA, MFT and ODI.\\n• Worked on developing and executing Integration Regression Test Script.\\n• Developing Application Understanding Documents for the Standard functionalities.\\n• Interacting with the Clients to understand the requirement and details of the issues and\\nproviding suggestions/answers to their queries.\\n• Resolving technical issues and enhancement raised by the customer.\\n• Reviewing work products from the Team Members and delivering products to the client on time\\nwith high quality.\\n• RCA for the Complex Issues in client's version of Oracle SOA and SCM.\\n• Working effectively with Oracle Product Support for any issue related to Standard Functionality.\\n• Co-ordinate internally with various teams, executives in providing the decisions to various\\nfactors influencing the project implementation\", 'annotation': [{'label': ['Companies worked at'], 'points': [{'start': 5365, 'end': 5370, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 5318, 'end': 5323, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 4676, 'end': 4681, 'text': 'Oracle'}]}, {'label': ['Designation'], 'points': [{'start': 4613, 'end': 4632, 'text': 'Technical Consultant'}]}, {'label': ['Companies worked at'], 'points': [{'start': 4407, 'end': 4412, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 4374, 'end': 4397, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 4374, 'end': 4379, 'text': 'Oracle'}]}, {'label': ['Designation'], 'points': [{'start': 4328, 'end': 4347, 'text': 'Technical Consultant'}]}, {'label': ['Companies worked at'], 'points': [{'start': 4210, 'end': 4215, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 4059, 'end': 4064, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 3449, 'end': 3454, 'text': 'Oracle'}]}, {'label': ['Designation'], 'points': [{'start': 3386, 'end': 3405, 'text': 'Technical Consultant'}]}, {'label': ['Companies worked at'], 'points': [{'start': 3165, 'end': 3170, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 3132, 'end': 3155, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 3132, 'end': 3137, 'text': 'Oracle'}]}, {'label': ['Designation'], 'points': [{'start': 3087, 'end': 3106, 'text': 'Technical Consultant'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2756, 'end': 2761, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2694, 'end': 2699, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2665, 'end': 2688, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2665, 'end': 2670, 'text': 'Oracle'}]}, {'label': [], 'points': [{'start': 2585, 'end': 2590, 'text': 'Oracle'}]}, {'label': ['Skills'], 'points': [{'start': 2562, 'end': 2567, 'text': 'Oracle'}]}, {'label': ['Skills'], 'points': [{'start': 2557, 'end': 2605, 'text': 'Xml, Oracle MFT, Core Java, Oracle SOA, WSDL, ODI'}]}, {'label': ['Graduation Year'], 'points': [{'start': 2543, 'end': 2546, 'text': '2015'}]}, {'label': ['College Name'], 'points': [{'start': 2482, 'end': 2532, 'text': 'CHHATTISGARH SWAMI VIVEKANANDA TECHNICAL UNIVERSITY'}]}, {'label': ['Degree'], 'points': [{'start': 2476, 'end': 2479, 'text': 'C.S.'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2407, 'end': 2430, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2407, 'end': 2412, 'text': 'Oracle'}]}, {'label': ['Years of Experience'], 'points': [{'start': 2394, 'end': 2402, 'text': '2.5 years'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2356, 'end': 2361, 'text': 'Oracle'}]}, {'label': ['Designation'], 'points': [{'start': 2334, 'end': 2353, 'text': 'Technical Consultant'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2184, 'end': 2207, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2184, 'end': 2189, 'text': 'Oracle'}]}, {'label': ['Designation'], 'points': [{'start': 2093, 'end': 2112, 'text': 'Technical Consultant'}]}, {'label': ['Companies worked at'], 'points': [{'start': 2086, 'end': 2091, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 1034, 'end': 1039, 'text': 'Oracle'}]}, {'label': ['Companies worked at'], 'points': [{'start': 556, 'end': 579, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 556, 'end': 561, 'text': 'Oracle'}]}, {'label': ['Years of Experience'], 'points': [{'start': 543, 'end': 551, 'text': '2.5 years'}]}, {'label': ['Graduation Year'], 'points': [{'start': 494, 'end': 497, 'text': '2015'}]}, {'label': ['Companies worked at'], 'points': [{'start': 452, 'end': 457, 'text': 'Oracle'}]}, {'label': ['Designation'], 'points': [{'start': 430, 'end': 449, 'text': 'Technical Consultant'}]}, {'label': ['Companies worked at'], 'points': [{'start': 263, 'end': 286, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 263, 'end': 268, 'text': 'Oracle'}]}, {'label': ['Location'], 'points': [{'start': 118, 'end': 125, 'text': 'Bilaspur'}]}, {'label': ['Companies worked at'], 'points': [{'start': 60, 'end': 83, 'text': 'Oracle Fusion Middleware'}]}, {'label': ['Companies worked at'], 'points': [{'start': 60, 'end': 65, 'text': 'Oracle'}]}, {'label': ['Years of Experience'], 'points': [{'start': 47, 'end': 55, 'text': '2.5 years'}]}, {'label': ['Name'], 'points': [{'start': 0, 'end': 15, 'text': 'Shrishti Chauhan'}]}], 'extras': None}\n",
            "There was an error parsing the label\n",
            "{'content': 'Debasish Dasgupta\\nTrainer-Finacle-Core Banking Solutions-Infosys - Onward eServices\\nlimited\\n\\nQasba, Bihar - Email me on Indeed: indeed.com/r/Debasish-Dasgupta/a20561e10f83ae3f\\n\\n✓ Worked as a faculty for Infosys in PTC ( Postal Training Centre) of DOP (Department of Post-\\nIndia)\\n✓ Undertaken Classes for Banking Theory and practices.\\n✓ Trainer- Finacle Core Banking Solutions\\n✓ Maintaining client base of 20 Crores.\\n✓ Investment Banking ( H.N.I section)\\n✓ Admin and ATM manager (IT and branch)\\n✓ Management and smooth functioning of ATM network and administration.\\n✓ Implementation of Finacle10.2 in the branch level (Axis Bank)\\n✓ Successfully completed the task of Transition Supervisor in the migration from Finacle7 to\\nFinacle 10.2.\\n✓ Over 4 years of rich experience in financial sector\\n✓ Ability to support and sustain a positive work environment that fosters team performance with\\nstrong communication and negotiation skills.\\n✓ Thorough understanding of cash management services involving bulk payments for large\\ncorporate and processing of cash and cheque collections from them.\\n✓ Thorough working knowledge Branch Banking &amp; routine banking operations.\\n✓ Possess excellent interpersonal, communication and organizational skills with demonstrated\\nabilities in team management and customer relationship management.\\n✓ Developing Customer base for the Bank and be strong player in the growth of overall business\\n\\nWORK EXPERIENCE\\n\\nTrainer-Finacle-Core Banking Solutions-Infosys\\n\\nOnward eServices limited -  Chennai, Tamil Nadu -\\n\\n2014 to Present\\n\\nJob Summary: Trainer in PTC (Postal Training centre) - Department of Post.\\n\\n• Training clients about Finacle CBS menu opttions.\\n• Training postal employees about the day to day banking practice.\\n• Training postal employees about Finacle 10.2 implementation in the department.\\n• Supervising in the migration from manual data entry to core banking application.\\n• Training client about the package.\\n• Migration of the system to new software.\\n\\nBranch Manager\\n\\nRainbow Financial Services -  Kolkata, West Bengal -\\n\\n2012 to April 2014\\n\\nhttps://www.indeed.com/r/Debasish-Dasgupta/a20561e10f83ae3f?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nInvestment Banking- salt Lake- Kolkata)\\nJob role: Branch Manager\\n\\nJob Summary:\\n• Cash flow management.\\n• Providing trading solutions to all clients.\\n• Providing working capital solutions to small scale and big firms.\\n• Treasury management.\\n• Worked as Financial Product consultant (H N I)\\n• Responsibility to provide financial solution to the customers\\n• Channelizing the liquidity of the customer to proper fund.\\n\\nAsst Manager\\n\\nAXIS Bank Ltd -\\n\\nJuly 2007 to 2012\\n\\nJamshedpur_\\n\\nJob Role: Admin and ATM coordinator of branch.\\n\\n* Job summary: Admin (Branch &amp; IT)\\n➢ Implementation of Finacle 10.2 (banking software) in the region.\\n➢ Training the branch staffs In Finacle 10.2 module and supervising the transition period of every\\ndepartment from Finacle 7 to Finacle 10.2. at the time of migration.\\n➢ Fixed asset management through FAMS software in the region assigned to me.\\n➢ Responsible of procurement of new assets and stationery through e-shop software, sale and\\npurchase of old assets through quotations from different vendors.\\n\\n➢ Space management of the branch level and 5\\'s implementation in the branch through proper\\ncoordination with the team.\\n➢ Attendance record management in the HR software.\\n➢ Maintenance of all important branch documents for e.g. personal HR file, govt. document\\nmaintained by all banks, all notices to be displayed in the branch.\\n➢ Attending security meetings with local authorities on fortnightly basis.\\n➢ Coordination with concurrent auditor and external auditor to ensure \"AAA\" ratings for the\\nBranch through proper compliance in yearly audit.\\n➢ Decision making authority of all petty expenses on a branch level.\\n➢ IT related issues to be coordinated with the help desk (WIPRO)\\n➢ Software and hardware management of all the branches in the region\\n➢ Network logbook maintenance and record keeping of all the issues faced by the branches.\\n➢ Record management of all the branch data and to keep a back up of the same in the main\\nserver.\\n➢ Responsible of smooth functioning of the server and network through regular track keeping\\nand ensuring all the service request or call lodged should attended within proper TAT.\\n* Job summary: ATM Manager:\\n\\n♦ Before outsourcing and centralization of ATM\\'s\\n➢ Reconciliation of the all ATM\\'s manually and in software.\\n\\n\\n\\n➢ Member of Circle Audit Team for compliance in ATM reconciliation of other branches.\\n➢ Reversal of all excess and short cash issues of the ATM\\'s.\\n➢ Maintaining TAT for own bank and other bank customer disputes on a regular basis.\\n➢ Coordination of ATM team and local logistics &amp; courier for smooth functioning of ATM.\\n➢ Keeping track of cash replenishment of all ATM\\'s in the region.\\n➢ Getting approval and maintaining record of all ATM related expenditure.\\n➢ Procurement of new ATM machine through different vendors and also to decide new location\\nof ATM site.\\n➢ Fortnight audit and verification of ATM\\'s and cash vaults\\n➢ Rent agreements of ATM\\'s and renewal of the same.\\n\\nAfter outsourcing and centralization of ATM\\'s\\n➢ Handing over the reconciliation of all the ATM\\'s to the central ATM nodal cell at the time of\\ncentralization of ATM\\'s.\\n➢ Coordination with outsource agency for smooth and proper functioning of ATM\\'s under the\\nbranch and also coordinating in acquisition of new ATM site.\\n➢ Daily updating of all the details in the respective software.\\n➢ Coordination with local CRA agency for smooth cash replenishment of all ATM\\'s.\\n➢ Coordination with the central reconciliation team to solve own bank and other bank disputes\\nwithin the given TAT.\\n\\n* Having through knowledge on implementation of Finacle7 &amp; Finacle10.2 (Core Banking\\nSolution Software developed by Infosys)\\n* Head cashier for 2.5 years, serving ATM cash Requirement, Cash remittance to currency Chest\\nand managing CDP (Cash delivery &amp; pick-up) by strictly following clean note policy.\\n* Handled Cash Management Services.\\n* Handled Front desk banking Operation for e.g. - D.D printing, cheque transfer, fixed deposit,\\nLocker Facility, GBM challan.\\n* Handling Customer Related Queries and KYC compliance and AML.\\n\\nKey Learning\\'s\\n• A complete training of Finacle 10.2 (Classroom and on the job)\\n• A holistic experience of overall administration, logistics and operation of banking procedures\\nto serve the HNI clients of the Bank and also to receive appreciation and acknowledgement from\\nboth the sides.\\n\\n• Thorough knowledge of Audit Compliance.\\n\\nFront Office Credit Coordinator\\n\\nH.D.F.C. Ltd -  Kolkata, West Bengal -\\n\\nSeptember 2005 to June 2007\\n\\nKolkata\\n\\nKey Result Areas across assignments:\\n\\nJob Responsibilities:\\n* Worked as Front Office Credit Coordinator.\\n\\n\\n\\n* Preparation of Housing loan Sanction report.\\n* Coordinating sales team and their sales promotional activities\\n\\nKey Learning\\'s\\n• Exposure to Housing Loan industry.\\n\\nFinancial Product consultant\\n\\nI.C.I.C.I PRUDENTIAL LIFE INSURANCE -  Kolkata, West Bengal -\\n\\nJanuary 2004 to February 2005\\n\\nH N I)\\n• Responsibility to provide financial solution to the customers\\n• Channelizing the liquidity of the customer to proper fund.\\n• different industries.\\n• Strong P.R build quality developed due to daily customer interaction.\\n• Successfully completed 5 years of banking as an ATM manager and branch admin.\\n• Many recognition from branch level and as well as circle level.\\n• Proper migration and implementation of banking software from Finacle 7 to Finacle10.2.\\n• Error free audit in Spam of 2.5 years as a Branch Main Cashier.\\n• Achievements Of targets in Third party Products.\\n• Managing complete branch operations with key focus on bottom line profitability by ensuring\\noptimal utilization of available resources.\\n• Implementation &amp; Achievement of 5S for the Branch.\\n\\nEDUCATION\\n\\nB.B.M\\n\\nAndhra University\\n\\n2003\\n\\nGulmohar High School -  Jamshedpur, Jharkhand\\n\\n2000', 'annotation': [{'label': [], 'points': [{'start': 7878, 'end': 7882, 'text': 'B.B.M'}]}, {'label': ['Companies worked at'], 'points': [{'start': 5840, 'end': 5846, 'text': 'Infosys'}]}, {'label': ['Email Address'], 'points': [{'start': 2090, 'end': 2136, 'text': 'indeed.com/r/Debasish-Dasgupta/a20561e10f83ae3f'}]}, {'label': ['Companies worked at'], 'points': [{'start': 1471, 'end': 1477, 'text': 'Infosys'}]}, {'label': ['Designation'], 'points': [{'start': 1432, 'end': 1469, 'text': 'Trainer-Finacle-Core Banking Solutions'}]}, {'label': ['Companies worked at'], 'points': [{'start': 203, 'end': 209, 'text': 'Infosys'}]}, {'label': ['Email Address'], 'points': [{'start': 128, 'end': 174, 'text': 'indeed.com/r/Debasish-Dasgupta/a20561e10f83ae3f'}]}, {'label': ['Location'], 'points': [{'start': 93, 'end': 97, 'text': 'Qasba'}]}, {'label': ['Companies worked at'], 'points': [{'start': 57, 'end': 63, 'text': 'Infosys'}]}, {'label': ['Designation'], 'points': [{'start': 18, 'end': 55, 'text': 'Trainer-Finacle-Core Banking Solutions'}]}, {'label': ['Name'], 'points': [{'start': 0, 'end': 16, 'text': 'Debasish Dasgupta'}]}], 'extras': None}\n",
            "218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy requires training data to be in 'Doc' objects. The output from the pipeline will also result in a Doc object. Finally the file will be saved in the runtime with the .spacy format."
      ],
      "metadata": {
        "id": "3FbHKXMuxeh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.training.example import Example\n",
        "from spacy.scorer import Scorer\n",
        "import random\n",
        "\n",
        "# Assuming formatted_data is a list of dictionaries with \"text\" and \"entities\" fields\n",
        "train_data = []\n",
        "for entry in formatted_data:\n",
        "    text = entry[\"text\"]\n",
        "    doc = nlp.make_doc(text)\n",
        "    entities = []\n",
        "    for entity in entry[\"entities\"]:\n",
        "        start, end, label = entity[\"start\"], entity[\"end\"], entity[\"label\"]\n",
        "        entities.append((start, end, label))\n",
        "    example = Example.from_dict(doc, {\"entities\": entities})\n",
        "    train_data.append(example)\n",
        "\n",
        "split_ratio = 0.8  # 80% for training, 20% for validation\n",
        "train_size = int(len(train_data) * split_ratio)\n",
        "train_set = train_data[:train_size]\n",
        "valid_set = train_data[train_size:]\n",
        "\n",
        "# Disable other pipelines during training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(12):  # Adjust the number of iterations as needed\n",
        "        print(\"Starting Iteration \" + str(itn))\n",
        "        losses = {}\n",
        "        random.shuffle(train_data)\n",
        "        nlp.update(train_data, sgd=optimizer, losses=losses)\n",
        "        print(losses)\n",
        "\n",
        "        scorer = Scorer()\n",
        "        scores = scorer.score(valid_set)\n",
        "        print(scores)"
      ],
      "metadata": {
        "id": "RCK3_MaBBwPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk(\"custom_ner_model\")"
      ],
      "metadata": {
        "id": "eE_qFFO-qlqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_updated = spacy.load(\"/content/custom_ner_model\")  # Replace \"path_to_your_trained_model\" with the actual path\n",
        "\n",
        "test_text = \"Abhishek Jha Application Development Associate Accenture Bengaluru Karnataka Email me on Indeed indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and companys growth in best possible ways Willing to relocate to Bangalore Karnataka WORK EXPERIENCE Application Development Associate Accenture November 2017 to Present Role Currently working on Chat bot Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input Also Training the bot for different possible utterances Both positive and negative which will be given as input by the user EDUCATION B.E in Information science and engineering B.v.b college of engineering and technology Hubli Karnataka August 2013 to June 2017 12th in Mathematics Woodbine modern school April 2011 to March 2013 10th Kendriya Vidyalaya April 2001 to March 2011 SKILLS C Less than 1 year Database Less than 1 year Database Management Less than 1 year Database Management System Less than 1 year Java Less than 1 year ADDITIONAL INFORMATION Technical Skills https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN Programming language C C++ Java Oracle PeopleSoft Internet Of Things Machine Learning Database Management System Computer Networks Operating System worked on Linux Windows Mac Non Technical Skills Honest and Hard Working Tolerant and Flexible to Different Situations Polite and Calm Team Player\"\n",
        "doc = nlp(test_text)\n",
        "\n",
        "print(len(doc.ents))\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
      ],
      "metadata": {
        "id": "EFHylzdyqrIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_doc_objects(entry):\n",
        "  text = entry[\"text\"]\n",
        "  doc = nlp.make_doc(text)\n",
        "  # Assuming you've already loaded the SpaCy model and created a 'doc' object from the text\n",
        "\n",
        "  annotations = entry[\"entities\"]\n",
        "\n",
        "  ents = []\n",
        "\n",
        "  for entity in annotations:\n",
        "      start = entity[\"start\"]\n",
        "      end = entity[\"end\"] + 1\n",
        "      label = entity[\"label\"]\n",
        "\n",
        "      # Initialize token indices\n",
        "      start_token_idx = None\n",
        "      end_token_idx = None\n",
        "\n",
        "      # Find the token indices that correspond to the character indices\n",
        "      for i, token in enumerate(doc):\n",
        "          if token.idx == start:\n",
        "              start_token_idx = i\n",
        "          if token.idx + len(token.text) == end:\n",
        "              end_token_idx = i\n",
        "\n",
        "      if start_token_idx is not None and end_token_idx is not None:\n",
        "          span = doc[start_token_idx:end_token_idx + 1]  # Create a span from tokens\n",
        "          span_label = (span.start_char, span.end_char, label)\n",
        "          span_to_add = doc.char_span(span.start_char, span.end_char, label=label)  # Format: (start_char, end_char, label)\n",
        "          ents.append(span_to_add)\n",
        "      else:\n",
        "          print(\"Invalid span detected:\", start, end)\n",
        "\n",
        "  # 'ents' will contain the entities in the format required for spaCy training\n",
        "  try:\n",
        "    doc.ents = ents\n",
        "    return doc\n",
        "  except ValueError:\n",
        "    print(\"Error while creating doc, skipping entry\")\n",
        "    return None\n",
        "\n",
        "db = DocBin()\n",
        "for entry in formatted_data:\n",
        "  doc_obj = create_doc_objects(entry)\n",
        "  if doc_obj != None:\n",
        "    db.add(doc_obj)\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONg9Pa-1_UH8",
        "outputId": "0239d8ad-fa8c-497d-b03f-a113cd4f44e1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid span detected: 900 1578\n",
            "Invalid span detected: 7303 7307\n",
            "Invalid span detected: 1790 2041\n",
            "Invalid span detected: 20 40\n",
            "Invalid span detected: 120 126\n",
            "Invalid span detected: 377 381\n",
            "Invalid span detected: 923 970\n",
            "Invalid span detected: 1029 1033\n",
            "Invalid span detected: 1711 1743\n",
            "Invalid span detected: 188 192\n",
            "Invalid span detected: 194 197\n",
            "Invalid span detected: 49 55\n",
            "Invalid span detected: 254 274\n",
            "Error while creating doc, skipping entry\n",
            "Invalid span detected: 57 101\n",
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 0 4\n",
            "Invalid span detected: 936 941\n",
            "Invalid span detected: 35 48\n",
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 5060 5115\n",
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 909 1028\n",
            "Invalid span detected: 0 17\n",
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 34 45\n",
            "Invalid span detected: 165 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.to_disk(\"./train.spacy\")"
      ],
      "metadata": {
        "id": "TVOiUdgEDhiV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "train_path = './train.spacy'\n",
        "\n",
        "print(\"Train file exists:\", os.path.exists(train_path))\n",
        "\n",
        "!python -m spacy init config - --lang en --pipeline ner --optimize efficiency > base_config.cfg\n",
        "#!python -m spacy train config.cfg --output ./output_directory --paths.train ./content/train.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dopt5UnP-P6e",
        "outputId": "b248c3b0-30ab-4403-ae8b-d25c7ddc30e9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train file exists: True\n",
            "2024-01-06 14:16:35.791355: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 14:16:35.791432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 14:16:35.792953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 14:16:37.172773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/base_config.cfg ./config.cfg"
      ],
      "metadata": {
        "id": "ebjU3sdCoDuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9056a27-fd18-40d1-eebc-41b1bec2cd03"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 14:16:52.906373: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 14:16:52.906440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 14:16:52.907929: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 14:16:54.795126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ Nothing to auto-fill: base config is already complete\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --paths.train /content/train.spacy --paths.dev /content/train.spacy --output output_folder"
      ],
      "metadata": {
        "id": "yLUI3IZmT-zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d30cb6-97e7-47eb-af97-7c2d92edfb54"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 14:17:04.400488: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 14:17:04.400567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 14:17:04.401861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 14:17:05.790662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory: output_folder\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    203.52    0.00    0.00    0.00    0.00\n",
            "  0     200       3259.91   8871.38   31.88   55.35   22.38    0.32\n",
            "  1     400       4000.82   3252.26   49.71   54.89   45.42    0.50\n",
            "  2     600       2874.86   2546.49   66.73   72.02   62.17    0.67\n",
            "  3     800      10474.03   2216.74   70.68   69.03   72.41    0.71\n",
            "  4    1000        453.70   1726.59   73.35   74.71   72.03    0.73\n",
            "  5    1200        922.16   1576.72   79.61   82.17   77.20    0.80\n",
            "  6    1400        951.68   1480.41   81.65   83.44   79.94    0.82\n",
            "  7    1600       3731.68   1309.42   84.35   87.54   81.39    0.84\n",
            "  8    1800        567.56   1168.56   87.22   91.47   83.34    0.87\n",
            "  9    2000        498.27   1029.80   86.00   85.13   86.88    0.86\n",
            " 10    2200        622.72    974.79   88.81   87.92   89.72    0.89\n",
            " 11    2400        485.56    811.29   89.11   90.70   87.58    0.89\n",
            " 12    2600       2249.98   1004.42   92.01   91.30   92.74    0.92\n",
            " 13    2800        621.64    700.57   91.29   91.41   91.16    0.91\n",
            " 14    3000        742.61    745.99   92.53   94.08   91.02    0.93\n",
            " 15    3200      13672.95    702.21   93.06   95.14   91.07    0.93\n",
            " 16    3400        815.25    754.94   93.96   92.82   95.11    0.94\n",
            " 17    3600       3414.03    710.28   92.33   92.01   92.65    0.92\n",
            " 18    3800        955.40    580.80   94.44   93.24   95.67    0.94\n",
            " 19    4000       1050.04    598.52   94.82   94.25   95.39    0.95\n",
            " 20    4200        876.62    573.76   93.57   94.41   92.74    0.94\n",
            " 21    4400        925.20    551.25   95.27   94.51   96.04    0.95\n",
            " 22    4600       1096.04    539.91   95.20   95.38   95.02    0.95\n",
            " 24    4800       1526.74    539.39   95.91   96.29   95.53    0.96\n",
            " 25    5000       1206.38    472.95   97.20   97.43   96.98    0.97\n",
            " 26    5200       1048.63    414.95   96.94   96.58   97.30    0.97\n",
            " 27    5400       1572.74    489.68   96.40   95.03   97.81    0.96\n",
            " 29    5600       1197.44    470.94   97.95   97.86   98.05    0.98\n",
            " 30    5800       1255.56    500.48   98.15   98.73   97.58    0.98\n",
            " 32    6000       1660.37    490.21   98.15   97.48   98.84    0.98\n",
            " 34    6200       1923.24    448.33   98.52   98.24   98.79    0.99\n",
            " 35    6400       1547.49    412.93   97.95   98.27   97.63    0.98\n",
            " 37    6600       2714.93    419.70   98.49   98.20   98.79    0.98\n",
            " 39    6800       1211.42    372.16   98.59   98.07   99.12    0.99\n",
            " 40    7000       1488.55    381.31   98.17   97.78   98.56    0.98\n",
            " 42    7200       2761.05    352.80   98.42   98.37   98.46    0.98\n",
            " 44    7400       1817.13    360.12   98.58   98.56   98.60    0.99\n",
            " 45    7600       1566.82    345.24   98.49   98.65   98.32    0.98\n",
            " 47    7800       1741.08    339.82   98.44   98.42   98.46    0.98\n",
            " 49    8000       1857.50    337.86   98.60   98.69   98.51    0.99\n",
            " 50    8200      27096.16    381.85   98.51   98.51   98.51    0.99\n",
            " 52    8400       2126.64    348.71   98.77   98.56   98.98    0.99\n",
            " 54    8600       2186.35    310.58   98.75   98.47   99.02    0.99\n",
            " 55    8800       1593.02    265.52   98.36   97.88   98.84    0.98\n",
            " 57    9000       1957.55    354.28   98.44   98.46   98.42    0.98\n",
            " 59    9200       1994.75    297.03   98.86   98.70   99.02    0.99\n",
            " 60    9400       1529.05    256.39   98.89   98.66   99.12    0.99\n",
            " 62    9600       1488.82    223.70   99.05   98.84   99.26    0.99\n",
            " 64    9800       2296.13    292.81   98.93   99.11   98.74    0.99\n",
            " 65   10000       1814.87    245.76   98.70   98.52   98.88    0.99\n",
            " 67   10200       2028.02    263.22   99.16   98.98   99.35    0.99\n",
            " 69   10400       1999.94    258.94   99.07   98.80   99.35    0.99\n",
            " 70   10600       2092.08    273.57   99.16   99.44   98.88    0.99\n",
            " 72   10800       1904.93    229.02   98.91   98.75   99.07    0.99\n",
            " 74   11000       1925.72    255.29   98.70   98.56   98.84    0.99\n",
            " 75   11200       2598.51    278.08   99.30   99.16   99.44    0.99\n",
            " 77   11400       1914.22    206.46   98.96   98.66   99.26    0.99\n",
            " 79   11600       2436.14    244.02   99.07   99.16   98.98    0.99\n",
            " 80   11800       3487.67    285.45   98.76   99.02   98.51    0.99\n",
            " 82   12000       3105.19    291.60   98.70   98.16   99.26    0.99\n",
            " 84   12200       2966.03    220.58   99.23   99.49   98.98    0.99\n",
            " 85   12400       2748.20    274.51   98.77   98.74   98.79    0.99\n",
            " 87   12600       3073.34    248.66   98.74   98.79   98.70    0.99\n",
            " 89   12800       3195.18    278.85   98.93   98.84   99.02    0.99\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output_folder/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_ner = spacy.load(\"/content/output_folder/model-best\")\n",
        "\n",
        "doc = nlp_ner(\"Abhishek Jha Application Development Associate Accenture Bengaluru Karnataka Email me on Indeed indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and companys growth in best possible ways Willing to relocate to Bangalore Karnataka WORK EXPERIENCE Application Development Associate Accenture November 2017 to Present Role Currently working on Chat bot Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input Also Training the bot for different possible utterances Both positive and negative which will be given as input by the user EDUCATION B.E in Information science and engineering B.v.b college of engineering and technology Hubli Karnataka August 2013 to June 2017 12th in Mathematics Woodbine modern school April 2011 to March 2013 10th Kendriya Vidyalaya April 2001 to March 2011 SKILLS C Less than 1 year Database Less than 1 year Database Management Less than 1 year Database Management System Less than 1 year Java Less than 1 year ADDITIONAL INFORMATION Technical Skills https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN Programming language C C++ Java Oracle PeopleSoft Internet Of Things Machine Learning Database Management System Computer Networks Operating System worked on Linux Windows Mac Non Technical Skills Honest and Hard Working Tolerant and Flexible to Different Situations Polite and Calm Team Player\")\n",
        "\n",
        "print(len(doc.ents))\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
        "\n",
        "\n",
        "#colors = {\"Name\": \"#F67DE3\", \"MEDICINE\": \"#7DF6D9\", \"MEDICALCONDITION\": \"#a6e22d\"}\n",
        "#options = {\"colors\": colors}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw_h0L_bUAwG",
        "outputId": "2769a91d-afb8-4fdb-9ad1-8c0bdaeb2fb5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "Entity: Abhishek Jha, Label: Name\n",
            "Entity: Application Development Associate, Label: Designation\n",
            "Entity: Accenture, Label: Companies worked at\n",
            "Entity: Bengaluru, Label: Location\n",
            "Entity: Indeed indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a, Label: Email Address\n",
            "Entity: 2017, Label: Graduation Year\n",
            "Entity: B.E in Information science and engineering, Label: Designation\n",
            "Entity: B.v.b college of engineering and technology, Label: College Name\n",
            "Entity: Woodbine modern school, Label: College Name\n",
            "Entity: Kendriya Vidyalaya, Label: College Name\n",
            "Entity: C Less than 1 year Database Less than 1 year Database Management Less than 1 year Database Management System Less than 1 year Java Less than 1 year, Label: Skills\n",
            "Entity: Programming language C C++ Java Oracle PeopleSoft Internet Of Things Machine Learning Database Management System Computer Networks Operating System worked on Linux Windows Mac Non Technical Skills Honest and Hard Working Tolerant and Flexible to Different Situations Polite and Calm Team Player, Label: Skills\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Includes references for all of the above sections of the notebook"
      ],
      "metadata": {
        "id": "mfdKFI73e88o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ghosh, S., Majumder, S. and Santosh Kumar Das. (2023). Artificial Intelligence Techniques in Human Resource Management. Apple Academic Press (CRC Press). Available from https://doi.org/10.1201/9781003328346.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Pawan Budhwar et al. (2023). Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT. Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT, 33 (3). Available from https://doi.org/10.1111/1748-8583.12524.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Rahmani, D. and Kamberaj, H. (2021). Implementation and Usage of Artificial Intelligence Powered Chatbots in Human Resources Management Systems. International Conference on Social and Applied Sciences. May 2021. Available from https://www.researchgate.net/profile/Hiqmet-Kamberaj/publication/351345726_Implementation_and_Usage_of_Artificial_Intelligence_Powered_Chatbots_in_Human_Resources_Management_Systems/links/60926106299bf1ad8d78e1d1/Implementation-and-Usage-of-Artificial-Intelligence-Powered-Chatbots-in-Human-Resources-Management-Systems.pdf [Accessed 19 December 2023].\n",
        "\n",
        "<br/>\n",
        "\n",
        "Rosenbaum, E. (2019). IBM artificial intelligence can predict with 95% accuracy which workers are about to quit their jobs. CNBC. Available from https://www.cnbc.com/2019/04/03/ibm-ai-can-predict-with-95-percent-accuracy-which-employees-will-quit.html [Accessed 1 January 2024].\n",
        "\n",
        "<br/>\n",
        "\n",
        "Tambe, P., Cappelli, P. and Yakubovich, V. (2019). Artificial Intelligence in Human Resources Management: Challenges and a Path Forward. California Management Review, 61 (4), 15–42. Available from https://doi.org/10.1177/0008125619867910.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Vrontis, D. et al. (2021). Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review. The International Journal of Human Resource Management, 33 (6), 1–30. Available from https://doi.org/10.1080/09585192.2020.1871398."
      ],
      "metadata": {
        "id": "zNQyVKnZg6KD"
      }
    }
  ]
}