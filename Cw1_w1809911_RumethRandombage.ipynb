{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Kg-JhFakYlkM",
        "OWvidz0phH8z",
        "mfdKFI73e88o"
      ],
      "authorship_tag": "ABX9TyNzIY5GtMeBgMqcS/KnbqI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RumethR/Resume-Ner/blob/main/Cw1_w1809911_RumethRandombage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Application Area Review\n",
        "\n",
        "**Selected Domain: Human resources and computing.**\n"
      ],
      "metadata": {
        "id": "Kg-JhFakYlkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The domain of human resources within an organisation entails a wide variety of operations and responsibilities ranging from employee compensation to organisational structure. This poses a great opportunity for Artificial Intelligent (AI) systems to contribute towards the Human Resource Management (HRM) tasks within an organisation. One of the most common areas where AI has been highly used is within the realm of recruitment and talent acquisition. A study by the ‘Oracle’ corporation has revealed that professionals in the human resource management industry believe that AI can boost productivity among other statistics (Rahmani and Kamberaj, 2021). Other sources believe that AI will enable new application procedures for candidates, increase applicant discovery rates and even reduce vacancy times as these systems can traverse through applications faster than trained professionals (Vrontis et al., 2021). AI powered applications in HRM are also able to process large amounts of data at high speeds, faster and more accurately than any trained professional, this allows organisations to gain insights that would otherwise be impossible with manual labour. The use of these systems are also more likely to be less biassed towards factors that do not affect performance such as age, gender or race (assuming that they are developed properly). Repetitive tasks within the domain of HRM are more likely to be successfully automated with AI powered applications according to *Vrontis et al. (2021)*.\n",
        "\n",
        "The overall effect of bias that is induced when training was studied by the influential work of *Tambe, Cappelli and Yakubovich (2019)* who observed that the data that these systems were built on had a major impact in how these systems performed. This underlying data itself has the opportunity to represent industry swings and unspoken ethics (i.e. the system might be trained on data that indicated towards hiring from a specific university), hence it is important that these systems work in tandem with trained professionals.This study also looks into how Machine Learning algorithms have fared against several different HR operations within organisations. AI powered HRM task management systems such as Quine, BenefitFocus and Jobvite are now widely available for any organisation of any scale to incorporate into their workflows (Tambe, Cappelli and Yakubovich, 2019).\n",
        "\n",
        "On the opposite side of the spectrum, employees now have access to tools such as IBM’s ‘Blue Match’ presents viable job openings for their specific skill set, similarly AI powered tools have helped IBM to identify possible flight risks and retain employees for much longer according to their former CEO Ginni Rometty (Rosenbaum, 2019). These HRM service vendors however do not provide any information about the type of AI techniques that are used internally within their products. It is also important to note that most AI powered solutions can typically be applied on a small portion of a HRM task, for example in the task of vetting candidates these tools can only provide a suggestion or score of desirability is most cases rather than performing the entire stack of recruiting tasks that go alongside the vetting process (Ghosh, Majumder and Santosh Kumar Das, 2023). The common theme among most studies that observe the usage of AI in HRM is that they are heavily reliant on the quality and the quantity of the data that they are being built upon.\n",
        "\n",
        "With the rise of open source Large Language Models (LLM) it is easier than ever for organisations to create a custom tailored algorithm that can perform trivial tasks within HRM such as answering frequently asked questions from employees through chatbots *Pawan Budhwar et al. (2023)* studied the use of popular generative AI tool ChatGPT in a HRM context and how it affects areas other than productivity such as employment relations, employee wellbeing and engagement. This study revealed that employees prefer tools such as ChatGPT in many scenarios that require understanding and perceiving information more than their human co-workers. However long term social, ethical and legal implications of using such technologies are difficult to quantify (Pawan Budhwar et al., 2023). Listed below are some other AI-based techniques used in the HRM context (Ghosh, Majumder and Santosh Kumar Das, 2023).\n",
        "\n",
        "*   Turnover Prediction with Artificial Neural Networks\n",
        "*   Candidate Search With Knowledge-Based Search Engines\n",
        "*   Sentiment Analysis on employee engagement and relations\n",
        "*   Résumé data acquisition using NLP\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PdkdKZArfPO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B: Comparision and evaluation of AI Techniques used in Human Resources"
      ],
      "metadata": {
        "id": "OWvidz0phH8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned before all HRM tasks in an organization cannot be supplemented or automated by one singular system. The scope of this specific section will cover the task of utilizing Machine Learning techniques for the purpose of recruiting, vetting and staffing employees using a CV/Résumé. This project will focus on using Named Entity Recognition (NER) on CVs to identify talent that best suites a given job posting. NER is classified as a Sequence Labelling Task within the domain of Natural Language Processing (NLP), there are many different ways of addressing these sequence labelling tasks listed below are three such approaches."
      ],
      "metadata": {
        "id": "itzimModhSzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using Conditional Random Fields (CRF)"
      ],
      "metadata": {
        "id": "m5YLiQ2f9-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Conditional Random Fields for NER tasks is a popular machine learning based approach which uses a probabilistic model. CRFs capture dependencies among neighbouring labels in a sequence and have the ability to consider the entire input sequence when making predictions. Feature functions are used to define relationships between input features (words) and output labels (entities). These relationships have to be manually defined by the trainer. CRFs calculate the probability distribution of an output sequence (labels) given an input sequence.Transition weights in Conditional Random Fields represent the likelihood of moving from one label to another in the sequence. During the training phase, the relationships between input features, their corresponding output labels (using data from the feature functions) and the effect of the transition weights are learned from the training data.\n",
        "\n",
        "For training a model that can perform NER using CRF text data with entities as labels are required. Necessary words in the training data sequence need to be accompanied by its corresponding entity label. Datasets for training NER models are widely available under different platforms such as Kaggle and the UCI Machine Learning Repository. One of the most common dataset and evaluation benchmark for NER is the CoNLL-2003 [dataset](https://www.clips.uantwerpen.be/conll2003/ner/). CRF based models tend to perform well as they have the ability to capture contextual information about the data. However performance is heavily dependent on the quality and relevance of engineered features, this is one of the major drawbacks of CRF based models. Feature engineering will mostly depend on the domain that the information is extracted from, and in the context of CVs people vary in the way they express information hence engineering features that capture necessary information will be quite a difficult task. Generally, CRF based models tend to be computationally efficient compared to their Deep Learning based counterparts who in turn will be better at capturing more intricate patterns within the data."
      ],
      "metadata": {
        "id": "YT0ru1sb-qMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using Neural Networks and pre-trained models"
      ],
      "metadata": {
        "id": "uQRcMfG2-IX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently there are many frameworks and pre-trained models that ease the task of NER. Similar to Tensorflow there are libraries that are built for performing NLP tasks. [SpaCy](https://spacy.io/usage/spacy-101) is one such Python package that offers pre-trained models for tasks such as NER. SpaCy’s default models use Convolutional Neural Networks (CNN) for the task of feature extraction and entity recognition. These models are trained on large labelled datasets hence they cover a wide range of contexts and features. By default the SpaCy package can identify entities within a  given text such as locations, dates, persons and organisations. These can be customised through a variety of API’s that are also included within the library. Similar to CRF and Transformer based models SpaCy’s CNNs can also understand contextual information regarding the input data, allowing the model to capture relationships between entities.\n",
        "\n",
        "Efficiency of a custom built CRF based model for NER depends heavily on the training data and the quality of the engineered features. Comparatively pre-trained models from SpaCy can offer similar accuracy scores and be more computationally efficient at  the same time. However by using a pre-built model the level of customization is limited. There are other libraries that have packaged in pre-trained models for NER such as [NLTK](https://www.nltk.org/) (Natural Language Toolkit) which can offer similar functionality to SpaCy, each library has their own strengths and weaknesses."
      ],
      "metadata": {
        "id": "5LRr3UGx_Jat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using pre-trained Transformer based models (BERT)"
      ],
      "metadata": {
        "id": "E399fVqg-Mmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the rise of pre-trained Transformer based models such as BERT and GPT, the application of these models for NLP tasks has proven to yield competitive results. The Bi-directional approach considers text preceding from both the left and the right of a given word in a sentence. This helps the model to understand a much broader context (and longer text sequences) compared to the default pre-trained models offered by SpaCy which uses a CNN. This can be especially useful in the context of parsing data from CVs which can contain ambiguous and complex sentences. There are many more pre-trained Transformer based models which can have this characteristic, these are available through the [Hugging Face](https://huggingface.co/) community platform. BERT specifically is trained on a large corpus of text using 2 unsupervised learning techniques, masked language modelling and next sentence prediction.\n",
        "\n",
        "Unlike SpaCy, BERT is not an out-of-the-box solution for most NER tasks. This model needs to be fine-tuned in order to be used for NER purposes. This allows BERT to be domain specific and can in theory lead to better performance on very complex and domain specific NER tasks. The drawback of this level of customization is that a BERT model is more resource intensive and technically challenging to develop compared to SpaCy or NLTK. It also requires a labelled dataset for training before it can be used. Fine-tuning a BERT model requires large amounts of data to yield competitive results, which might not be publicly available in the case of CVs. In summary, BERT models can perform better than the other two approaches given that they are fine-tuned correctly, whilst SpaCy offers a better efficiency to performance ratio without requiring extensive training or customization. Although compared to training a CRF based model for NER form scratch customising a BERT model will yield better results, one drawback is it being more computationally intensive to train and use.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nf2NHgPdLZsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C: Implementation"
      ],
      "metadata": {
        "id": "bbGLUtHfjtY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using a text-based resume dataset available from Kaggle, the dataset will be accessed directly via this notebook using the Kaggle API. The credentials are hardcoded onto this notebook for ease-of-use during the viva. (Not Recommended)"
      ],
      "metadata": {
        "id": "HQY7HRr3tl_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"rumethrandombage\",\"key\":\"6c49cc180f0498fccf2dabb3d22f0236\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "import json\n",
        "\n",
        "!kaggle datasets download -d dataturks/resume-entities-for-ner\n",
        "!unzip resume-entities-for-ner.zip #Extracts to ./content\n",
        "\n",
        "dataset = [] # Each element contains a json object\n",
        "with open('/content/Entity Recognition in Resumes.json', 'r') as file:\n",
        "    for line in file:\n",
        "        dataset.append(json.loads(line))\n",
        "\n",
        "print(\"Number of elements in dataset list\", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I-uBUKswCVZ",
        "outputId": "1c43409c-9110-4228-ec65-3d55cb826322"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Downloading resume-entities-for-ner.zip to /content\n",
            "  0% 0.00/323k [00:00<?, ?B/s]\n",
            "100% 323k/323k [00:00<00:00, 14.6MB/s]\n",
            "Archive:  resume-entities-for-ner.zip\n",
            "  inflating: Entity Recognition in Resumes.json  \n",
            "Number of elements in dataset list 220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 220 JSON objects. The structure of each JSON object is as follows:\n",
        "\n",
        "> \"content\" -> (contains all the data in the CV as a string)\n",
        "\n",
        "> \"annotations\" -> annotation label (\"Name\", \"Skills\", etc.) -> points (\"start and stop index where the label is contained\") - > text for label\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HpyO5Zi-CGmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy is packaged with a CNN based pre-built model that can perform NER for general entity types (Persons, Organizations, Locations, etc.)."
      ],
      "metadata": {
        "id": "XftpgsXqj1BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdJX7D5damP1",
        "outputId": "6765a6cc-9c9b-4e29-d829-1a150c7b2206"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 12:33:53.631246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 12:33:53.631315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 12:33:53.632911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 12:33:53.641642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 12:33:55.000448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Access one string of text without any annotations from the datset and pass it into the pre-trained spacy model\n",
        "first_entry = dataset[0]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(first_entry[\"content\"])\n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True,)\n",
        "#GPE stands for Geo-Political Entity (Place)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VCYOyakvCGJN",
        "outputId": "2c097a1c-8e05-4a11-9945-9d04e073b96c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Abhishek Jha\n",
              "Application\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " Development Associate - Accenture<br><br>\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengaluru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Karnataka - Email\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a<br><br>• To work for an organization which provides me the opportunity to improve my skills<br>and knowledge for my individual and company's growth in best possible ways.<br><br>Willing to relocate to: Bangalore, \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Karnataka\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "<br><br>WORK EXPERIENCE<br><br>Application Development Associate<br><br>Accenture -<br><br>\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    November 2017\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " to Present<br><br>Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries<br>for the Bot which will be triggered based on given input. Also, Training the bot for different possible<br>utterances (Both positive and negative), which will be given as<br>input by the user.<br><br>EDUCATION<br><br>\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    B.E in Information\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " science and engineering<br><br>\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    B.v.b\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " college of engineering and technology -  \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hubli\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Karnataka\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "<br><br>\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    August 2013 to June 2017\n",
              "\n",
              "12th\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mathematics\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              "<br><br>Woodbine modern school<br><br>\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    April 2011 to March 2013\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "<br><br>\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    10th\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              "<br><br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Kendriya Vidyalaya\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "<br><br>\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    April 2001 to March 2011\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "<br><br>SKILLS<br><br>C (\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Less than 1 year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "), Database (\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Less than 1 year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "), \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Database Management\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Less than 1 year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "),<br>\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Database Management System\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Less than 1 year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "), \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Java\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Less than 1 year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ")<br><br>ADDITIONAL INFORMATION<br><br>Technical Skills<br><br>https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&amp;ikw=download-top&amp;co=IN<br><br><br>\n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    • Programming\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              " language: C, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    C++\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Java\n",
              "• Oracle\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " PeopleSoft<br>• Internet Of Things<br>• Machine Learning<br>• Database Management System<br>\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    • Computer Networks\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "<br>• Operating System worked on: \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Linux\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Windows\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", Mac<br><br>\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Non - Technical Skills\n",
              "\n",
              "• Honest\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and Hard-Working<br>• Tolerant and Flexible to Different Situations<br>• Polite and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Calm\n",
              "• Team-Player\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The built-in NER model performs extremely poorly without any customisations. We can use transfer learning or we can create our own model using the NER pipeline provided by SpaCy, we will be creating a custom model for this project."
      ],
      "metadata": {
        "id": "HVhLRec3VUk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm # Used to show progress bars also helps identify places with problems\n",
        "from spacy.util import filter_spans # To make sure there are no overlapping entities\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "labels = [\"Name\", \"College Name\", \"Degree\", \"Graduation Year\", \"Years of Experience\", \"Companies worked at\", \"Designation\", \"Skills\", \"Location\", \"Email Address\"]\n",
        "\n",
        "#ner = nlp.add_pipe(\"ner\") #only using the ner component for the pipeline provided by SpaCy\n",
        "ner = nlp.get_pipe('ner')\n",
        "\n",
        "# Add the pre-defined labels gathered from the dataset to the pipeline\n",
        "for label in labels:\n",
        "  ner.add_label(label)"
      ],
      "metadata": {
        "id": "oEso7kj8tZx7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required format for SpaCy to train model.\n",
        "\n",
        "{\n",
        "    \"text\": \"Apple Inc. is headquartered in California.\",\n",
        "    \"entities\": [\n",
        "        {\"start\": 0, \"end\": 10, \"label\": \"ORG\"},\n",
        "        {\"start\": 36, \"end\": 46, \"label\": \"GPE\"}\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "5OAbpP-dpxnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current format (After extracting from dataset)\n",
        "\n",
        "{\n",
        "  \"content\": \"Rumeth Randombage is a student at IIT, with one year Internship experience. Email .....\",\n",
        "\n",
        "  annotation: [\n",
        "    { [label: \"Skills\"],  \n",
        "      [points:\n",
        "      {start: 1295,\n",
        "      end: 1621,\n",
        "      text: Python}\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "A3gPGSPAp8RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# Convert the data to spaCy's training data format\n",
        "formatted_data = []\n",
        "\n",
        "def process_dataset_entry(first_entry):\n",
        "  raw_text = first_entry[\"content\"]\n",
        "\n",
        "  # Clean text: remove punctuation and newline characters\n",
        "  cleaned_text = re.sub(r'[\\n\\r]', ' ', raw_text)\n",
        "  cleaned_text = cleaned_text.replace(\"'\", \"\")\n",
        "\n",
        "  #cleaned_text = raw_text.translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ')\n",
        "\n",
        "  # Process cleaned text with spaCy\n",
        "  doc = nlp.make_doc(cleaned_text)\n",
        "\n",
        "  # Retrieve cleaned text without punctuation and newline characters\n",
        "  cleaned_text = ' '.join(token.text for token in doc if not token.is_punct and not token.is_space)\n",
        "\n",
        "  formatted_entry = {\"text\": \"\", \"entities\": []}\n",
        "  formatted_entry[\"text\"] = cleaned_text\n",
        "\n",
        "  # Make sure that the given start and end points for specific label lines up with the text\n",
        "  entities = []\n",
        "\n",
        "  # This loop checks if the labels and annotations are correct.\n",
        "  for i in first_entry[\"annotation\"]:\n",
        "    try:\n",
        "      label = i['label'][0]\n",
        "      annotation_text = i[\"points\"][0][\"text\"]\n",
        "\n",
        "      #clean the annotation text as before, and get the start and end points within the larger text\n",
        "      annotation_text_clean = re.sub(r'[\\n\\r]', ' ', annotation_text)\n",
        "      annotation_text_clean = annotation_text_clean.replace(\"'\", \"\")\n",
        "\n",
        "      # Process cleaned text with spaCy\n",
        "      doc = nlp.make_doc(annotation_text_clean)\n",
        "\n",
        "      # Retrieve cleaned text without punctuation and newline characters\n",
        "      cleaned_annotation_text = ' '.join(token.text for token in doc if not token.is_punct and not token.is_space)\n",
        "\n",
        "      start_index = cleaned_text.find(cleaned_annotation_text)\n",
        "      end_index = start_index + len(cleaned_annotation_text) - 1 if start_index != -1 else -1\n",
        "      if label in labels: # In case there are some labels that are not needed/wrong\n",
        "        entities.append({\"start\": start_index, \"end\": end_index, \"label\": label})\n",
        "        formatted_entry[\"entities\"] = entities\n",
        "    except IndexError:\n",
        "      print(\"There was an error parsing a label, skipping entity\")\n",
        "      return\n",
        "\n",
        "  discard_entries = 0 # Discarded because of overlapping\n",
        "  checked_entry = prevent_overlapping(formatted_entry)\n",
        "  if len(checked_entry[\"entities\"]) == 0:\n",
        "    print(\"There was overlapping within all the entities\")\n",
        "    discard_entries = discard_entries + 1\n",
        "    return\n",
        "  else:\n",
        "    formatted_data.append(formatted_entry)\n",
        "\n",
        "\n",
        "# Make sure the entities do not overlap within the same text span\n",
        "def prevent_overlapping(entry):\n",
        "  # Overlapping refers to when two labels point to the same text span\n",
        "  entities = entry[\"entities\"]\n",
        "\n",
        "  # Sort entities by start index to check for overlaps\n",
        "  sorted_entities = sorted(entities, key=lambda x: x['start'])\\\n",
        "\n",
        "  previous_end = -1\n",
        "  non_overlapping_entities = []\n",
        "  overlapping_entities = []\n",
        "  for entity in sorted_entities:\n",
        "    if entity['start'] >= previous_end:\n",
        "        non_overlapping_entities.append(entity)\n",
        "        previous_end = entity['end']\n",
        "    else:\n",
        "      overlapping_entities.append(entry)\n",
        "      #print(\"Overlapping Detected on the following entity\")\n",
        "      #print(entity)\n",
        "\n",
        "  # Create and return a new entry with the only the non overlapping entities\n",
        "  if len(non_overlapping_entities) >= 1:\n",
        "    entry[\"entities\"] = non_overlapping_entities\n",
        "    return entry\n",
        "\n",
        "for entry in tqdm(dataset):\n",
        "  process_dataset_entry(entry)\n",
        "\n",
        "#print(formatted_data[0])\n",
        "print(len(formatted_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiUV0RRdnTHS",
        "outputId": "a1cdbfab-0117-4b87-df1d-55a4e004392c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 88/220 [00:01<00:02, 56.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There was an error parsing a label, skipping entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 185/220 [00:02<00:00, 80.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There was an error parsing a label, skipping entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:03<00:00, 66.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy requires training data to be in 'Doc' objects. The output from the pipeline will also result in a Doc object. Finally the file will be saved in the runtime with the .spacy format."
      ],
      "metadata": {
        "id": "3FbHKXMuxeh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_doc_objects(entry):\n",
        "  text = entry[\"text\"]\n",
        "  doc = nlp.make_doc(text)\n",
        "  # Assuming you've already loaded the SpaCy model and created a 'doc' object from the text\n",
        "\n",
        "  annotations = entry[\"entities\"]\n",
        "\n",
        "  ents = []\n",
        "\n",
        "  for entity in annotations:\n",
        "      start = entity[\"start\"]\n",
        "      end = entity[\"end\"] + 1\n",
        "      label = entity[\"label\"]\n",
        "\n",
        "      # Initialize token indices\n",
        "      start_token_idx = None\n",
        "      end_token_idx = None\n",
        "\n",
        "      # Find the token indices that correspond to the character indices\n",
        "      for i, token in enumerate(doc):\n",
        "          if token.idx == start:\n",
        "              start_token_idx = i\n",
        "          if token.idx + len(token.text) == end:\n",
        "              end_token_idx = i\n",
        "\n",
        "      if start_token_idx is not None and end_token_idx is not None:\n",
        "          span = doc[start_token_idx:end_token_idx + 1]  # Create a span from tokens\n",
        "          span_label = (span.start_char, span.end_char, label)\n",
        "          span_to_add = doc.char_span(span.start_char, span.end_char, label=label)  # Format: (start_char, end_char, label)\n",
        "          ents.append(span_to_add)\n",
        "      else:\n",
        "          print(\"Invalid span detected:\", start, end)\n",
        "\n",
        "  # 'ents' will contain the entities in the format required for spaCy training\n",
        "  try:\n",
        "    doc.ents = ents\n",
        "    return doc\n",
        "  except ValueError:\n",
        "    print(\"Error while creating doc, skipping entry\")\n",
        "    return None\n",
        "\n",
        "db = DocBin()\n",
        "for entry in tqdm(formatted_data):\n",
        "  doc_obj = create_doc_objects(entry)\n",
        "  if doc_obj != None:\n",
        "    db.add(doc_obj)\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONg9Pa-1_UH8",
        "outputId": "286d9b13-3305-41e5-bdf8-3dd7f7ae190e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 24/218 [00:00<00:01, 115.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid span detected: 900 1578\n",
            "Invalid span detected: 7303 7307\n",
            "Invalid span detected: 1790 2041\n",
            "Invalid span detected: 20 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 51/218 [00:00<00:01, 121.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid span detected: 120 126\n",
            "Invalid span detected: 377 381\n",
            "Invalid span detected: 923 970\n",
            "Invalid span detected: 1029 1033\n",
            "Invalid span detected: 1711 1743\n",
            "Invalid span detected: 188 192\n",
            "Invalid span detected: 194 197\n",
            "Invalid span detected: 49 55\n",
            "Invalid span detected: 254 274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 99/218 [00:00<00:01, 102.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error while creating doc, skipping entry\n",
            "Invalid span detected: 57 101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 153/218 [00:01<00:00, 159.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 0 4\n",
            "Invalid span detected: 936 941\n",
            "Invalid span detected: 35 48\n",
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 5060 5115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 190/218 [00:01<00:00, 156.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 909 1028\n",
            "Invalid span detected: 0 17\n",
            "Invalid span detected: -1 0\n",
            "Invalid span detected: 34 45\n",
            "Invalid span detected: 165 173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 218/218 [00:01<00:00, 131.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "docs = list(db.get_docs(nlp.vocab))\n",
        "\n",
        "# Shuffle the Doc objects\n",
        "random.seed(42)  # Set seed for reproducibility\n",
        "random.shuffle(docs)\n",
        "\n",
        "# Define the ratio for train-test split (e.g., 80% train, 20% test)\n",
        "train_ratio = 0.8\n",
        "num_docs = len(docs)\n",
        "train_size = int(train_ratio * num_docs)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_docs = docs[:train_size]\n",
        "test_docs = docs[train_size:]\n",
        "\n",
        "print(\"Number of Valid and usable data points for training: \", len(train_docs))\n",
        "print(\"Number of Valid and usable data points for training: \", len(test_docs))\n",
        "\n",
        "train_doc_bin = DocBin(docs=train_docs)\n",
        "test_doc_bin = DocBin(docs=test_docs)\n",
        "\n",
        "# Save the train and test DocBins to separate files\n",
        "train_doc_bin.to_disk(\"./train_data.spacy\")\n",
        "test_doc_bin.to_disk(\"./test.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVOiUdgEDhiV",
        "outputId": "d0c98237-9902-4b7f-df42-8081df8794e4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Valid and usable data points for training:  173\n",
            "Number of Valid and usable data points for training:  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "train_path = './train.spacy'\n",
        "test_path = \"./test.spacy\"\n",
        "\n",
        "print(\"Train file exists:\", os.path.exists(train_path))\n",
        "print(\"Test file exists:\", os.path.exists(train_path))\n",
        "\n",
        "!python -m spacy init config - --lang en --pipeline ner --optimize efficiency > config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dopt5UnP-P6e",
        "outputId": "b248c3b0-30ab-4403-ae8b-d25c7ddc30e9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train file exists: True\n",
            "2024-01-06 14:16:35.791355: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 14:16:35.791432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 14:16:35.792953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 14:16:37.172773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The config.cfg file contains all the necessary hyperparameters to the model. This uses the default configuration. The optimizer is set to **Adam** by default, and the **learning rate to 0.001**."
      ],
      "metadata": {
        "id": "K9F4Dqa5Z8jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING: RUNNING THIS MIGHT TAKE A WHILE ~40mins\n",
        "!python -m spacy train config.cfg --paths.train /content/train.spacy --paths.dev /content/test.spacy --output output_folder"
      ],
      "metadata": {
        "id": "yLUI3IZmT-zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d30cb6-97e7-47eb-af97-7c2d92edfb54"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-06 14:17:04.400488: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 14:17:04.400567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 14:17:04.401861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 14:17:05.790662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory: output_folder\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    203.52    0.00    0.00    0.00    0.00\n",
            "  0     200       3259.91   8871.38   31.88   55.35   22.38    0.32\n",
            "  1     400       4000.82   3252.26   49.71   54.89   45.42    0.50\n",
            "  2     600       2874.86   2546.49   66.73   72.02   62.17    0.67\n",
            "  3     800      10474.03   2216.74   70.68   69.03   72.41    0.71\n",
            "  4    1000        453.70   1726.59   73.35   74.71   72.03    0.73\n",
            "  5    1200        922.16   1576.72   79.61   82.17   77.20    0.80\n",
            "  6    1400        951.68   1480.41   81.65   83.44   79.94    0.82\n",
            "  7    1600       3731.68   1309.42   84.35   87.54   81.39    0.84\n",
            "  8    1800        567.56   1168.56   87.22   91.47   83.34    0.87\n",
            "  9    2000        498.27   1029.80   86.00   85.13   86.88    0.86\n",
            " 10    2200        622.72    974.79   88.81   87.92   89.72    0.89\n",
            " 11    2400        485.56    811.29   89.11   90.70   87.58    0.89\n",
            " 12    2600       2249.98   1004.42   92.01   91.30   92.74    0.92\n",
            " 13    2800        621.64    700.57   91.29   91.41   91.16    0.91\n",
            " 14    3000        742.61    745.99   92.53   94.08   91.02    0.93\n",
            " 15    3200      13672.95    702.21   93.06   95.14   91.07    0.93\n",
            " 16    3400        815.25    754.94   93.96   92.82   95.11    0.94\n",
            " 17    3600       3414.03    710.28   92.33   92.01   92.65    0.92\n",
            " 18    3800        955.40    580.80   94.44   93.24   95.67    0.94\n",
            " 19    4000       1050.04    598.52   94.82   94.25   95.39    0.95\n",
            " 20    4200        876.62    573.76   93.57   94.41   92.74    0.94\n",
            " 21    4400        925.20    551.25   95.27   94.51   96.04    0.95\n",
            " 22    4600       1096.04    539.91   95.20   95.38   95.02    0.95\n",
            " 24    4800       1526.74    539.39   95.91   96.29   95.53    0.96\n",
            " 25    5000       1206.38    472.95   97.20   97.43   96.98    0.97\n",
            " 26    5200       1048.63    414.95   96.94   96.58   97.30    0.97\n",
            " 27    5400       1572.74    489.68   96.40   95.03   97.81    0.96\n",
            " 29    5600       1197.44    470.94   97.95   97.86   98.05    0.98\n",
            " 30    5800       1255.56    500.48   98.15   98.73   97.58    0.98\n",
            " 32    6000       1660.37    490.21   98.15   97.48   98.84    0.98\n",
            " 34    6200       1923.24    448.33   98.52   98.24   98.79    0.99\n",
            " 35    6400       1547.49    412.93   97.95   98.27   97.63    0.98\n",
            " 37    6600       2714.93    419.70   98.49   98.20   98.79    0.98\n",
            " 39    6800       1211.42    372.16   98.59   98.07   99.12    0.99\n",
            " 40    7000       1488.55    381.31   98.17   97.78   98.56    0.98\n",
            " 42    7200       2761.05    352.80   98.42   98.37   98.46    0.98\n",
            " 44    7400       1817.13    360.12   98.58   98.56   98.60    0.99\n",
            " 45    7600       1566.82    345.24   98.49   98.65   98.32    0.98\n",
            " 47    7800       1741.08    339.82   98.44   98.42   98.46    0.98\n",
            " 49    8000       1857.50    337.86   98.60   98.69   98.51    0.99\n",
            " 50    8200      27096.16    381.85   98.51   98.51   98.51    0.99\n",
            " 52    8400       2126.64    348.71   98.77   98.56   98.98    0.99\n",
            " 54    8600       2186.35    310.58   98.75   98.47   99.02    0.99\n",
            " 55    8800       1593.02    265.52   98.36   97.88   98.84    0.98\n",
            " 57    9000       1957.55    354.28   98.44   98.46   98.42    0.98\n",
            " 59    9200       1994.75    297.03   98.86   98.70   99.02    0.99\n",
            " 60    9400       1529.05    256.39   98.89   98.66   99.12    0.99\n",
            " 62    9600       1488.82    223.70   99.05   98.84   99.26    0.99\n",
            " 64    9800       2296.13    292.81   98.93   99.11   98.74    0.99\n",
            " 65   10000       1814.87    245.76   98.70   98.52   98.88    0.99\n",
            " 67   10200       2028.02    263.22   99.16   98.98   99.35    0.99\n",
            " 69   10400       1999.94    258.94   99.07   98.80   99.35    0.99\n",
            " 70   10600       2092.08    273.57   99.16   99.44   98.88    0.99\n",
            " 72   10800       1904.93    229.02   98.91   98.75   99.07    0.99\n",
            " 74   11000       1925.72    255.29   98.70   98.56   98.84    0.99\n",
            " 75   11200       2598.51    278.08   99.30   99.16   99.44    0.99\n",
            " 77   11400       1914.22    206.46   98.96   98.66   99.26    0.99\n",
            " 79   11600       2436.14    244.02   99.07   99.16   98.98    0.99\n",
            " 80   11800       3487.67    285.45   98.76   99.02   98.51    0.99\n",
            " 82   12000       3105.19    291.60   98.70   98.16   99.26    0.99\n",
            " 84   12200       2966.03    220.58   99.23   99.49   98.98    0.99\n",
            " 85   12400       2748.20    274.51   98.77   98.74   98.79    0.99\n",
            " 87   12600       3073.34    248.66   98.74   98.79   98.70    0.99\n",
            " 89   12800       3195.18    278.85   98.93   98.84   99.02    0.99\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output_folder/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENTS_F = F1 Score for each epoch in indentifying the entities\n",
        "\n",
        "ENTS_P = F1 Score for each epoch in indentifying the entities\n",
        "\n",
        "ENTS_R = F1 Score for each epoch in indentifying the entities"
      ],
      "metadata": {
        "id": "-F5vgqLMc9rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_ner = spacy.load(\"/content/output_folder/model-best\")"
      ],
      "metadata": {
        "id": "pw_h0L_bUAwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if the model works, using a datapoint in the dataset."
      ],
      "metadata": {
        "id": "9vQjfxDKjhBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp_ner(\"Abhishek Jha Application Development Associate Accenture Bengaluru Karnataka Email me on Indeed indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and companys growth in best possible ways Willing to relocate to Bangalore Karnataka WORK EXPERIENCE Application Development Associate Accenture November 2017 to Present Role Currently working on Chat bot Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input Also Training the bot for different possible utterances Both positive and negative which will be given as input by the user EDUCATION B.E in Information science and engineering B.v.b college of engineering and technology Hubli Karnataka August 2013 to June 2017 12th in Mathematics Woodbine modern school April 2011 to March 2013 10th Kendriya Vidyalaya April 2001 to March 2011 SKILLS C Less than 1 year Database Less than 1 year Database Management Less than 1 year Database Management System Less than 1 year Java Less than 1 year ADDITIONAL INFORMATION Technical Skills https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN Programming language C C++ Java Oracle PeopleSoft Internet Of Things Machine Learning Database Management System Computer Networks Operating System worked on Linux Windows Mac Non Technical Skills Honest and Hard Working Tolerant and Flexible to Different Situations Polite and Calm Team Player\")\n",
        "\n",
        "colors = {\n",
        "    \"Name\": \"#ffcccb\",  # Name - light red\n",
        "    \"College Name\": \"#ffebcd\",  # College Name - blanched almond\n",
        "    \"Degree\": \"#add8e6\",  # Degree - light blue\n",
        "    \"Graduation Year\": \"#98fb98\",  # Graduation Year - pale green\n",
        "    \"Years of Experience\": \"#ffb6c1\",  # Years of Experience - light pink\n",
        "    \"Companies worked at\": \"#ffdead\",  # Companies worked at - navajo white\n",
        "    \"Designation\": \"#ffe4e1\",  # Designation - misty rose\n",
        "    \"Skills\": \"#f0e68c\",  # Skills - brown\n",
        "    \"Location\": \"#afeeee\",  # Location - pale turquoise\n",
        "    \"Email Address\": \"#d3d3d3\"  # Email Address - light gray\n",
        "}\n",
        "\n",
        "# Visualize named entities using displacy with custom colors for all labels\n",
        "options = {\"ents\": labels, \"colors\": colors}\n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True, options = options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "dCGMdWzeZpOV",
        "outputId": "7736b00d-d990-4334-c894-9dbbb2614168"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ffcccb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Abhishek Jha\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Name</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ffe4e1; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Application Development Associate\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Designation</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ffdead; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Accenture\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Companies worked at</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #afeeee; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengaluru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
              "</mark>\n",
              " Karnataka Email me on \n",
              "<mark class=\"entity\" style=\"background: #d3d3d3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Indeed indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Email Address</span>\n",
              "</mark>\n",
              " To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and companys growth in best possible ways Willing to relocate to Bangalore Karnataka WORK EXPERIENCE Application Development Associate Accenture November \n",
              "<mark class=\"entity\" style=\"background: #98fb98; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2017\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Graduation Year</span>\n",
              "</mark>\n",
              " to Present Role Currently working on Chat bot Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input Also Training the bot for different possible utterances Both positive and negative which will be given as input by the user EDUCATION \n",
              "<mark class=\"entity\" style=\"background: #ffe4e1; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    B.E in Information science and engineering\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Designation</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ffebcd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    B.v.b college of engineering and technology\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">College Name</span>\n",
              "</mark>\n",
              " Hubli Karnataka August 2013 to June 2017 12th in Mathematics \n",
              "<mark class=\"entity\" style=\"background: #ffebcd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Woodbine modern school\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">College Name</span>\n",
              "</mark>\n",
              " April 2011 to March 2013 10th \n",
              "<mark class=\"entity\" style=\"background: #ffebcd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Kendriya Vidyalaya\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">College Name</span>\n",
              "</mark>\n",
              " April 2001 to March 2011 SKILLS \n",
              "<mark class=\"entity\" style=\"background: #f0e68c; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    C Less than 1 year Database Less than 1 year Database Management Less than 1 year Database Management System Less than 1 year Java Less than 1 year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Skills</span>\n",
              "</mark>\n",
              " ADDITIONAL INFORMATION Technical Skills https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&amp;ikw=download-top&amp;co=IN \n",
              "<mark class=\"entity\" style=\"background: #f0e68c; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Programming language C C++ Java Oracle PeopleSoft Internet Of Things Machine Learning Database Management System Computer Networks Operating System worked on Linux Windows Mac Non Technical Skills Honest and Hard Working Tolerant and Flexible to Different Situations Polite and Calm Team Player\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Skills</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.training.example import Example\n",
        "\n",
        "# Load the test data from the .spacy file\n",
        "test_data = DocBin().from_disk(\"/content/test.spacy\")  # Replace with your file path\n",
        "\n",
        "# Convert DocBin back to individual Doc objects\n",
        "test_docs = list(test_data.get_docs(nlp.vocab))\n",
        "\n",
        "# Initialize a list to store Example objects for evaluation\n",
        "eval_examples = []\n",
        "\n",
        "# Create Example objects from the test documents\n",
        "for doc in test_docs:\n",
        "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "    example = Example.from_dict(doc, {\"entities\": entities})\n",
        "    eval_examples.append(example)\n",
        "\n",
        "# Evaluate the model with the created Example objects\n",
        "scores = nlp.evaluate(eval_examples)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK0z05dPduu4",
        "outputId": "bd0a46af-c8b9-42c0-c499-781e0975e820"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'tag_acc': None, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'pos_acc': None, 'morph_acc': None, 'morph_micro_p': None, 'morph_micro_r': None, 'morph_micro_f': None, 'morph_per_feat': None, 'lemma_acc': None, 'ents_p': 0.1783142736128759, 'ents_r': 1.0, 'ents_f': 0.30265995686556435, 'ents_per_type': {'Name': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'Designation': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'Companies worked at': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'Location': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'Email Address': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'ORG': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'Graduation Year': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'DATE': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'Degree': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'College Name': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'PERSON': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'Skills': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'CARDINAL': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'WORK_OF_ART': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'NORP': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'TIME': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'FAC': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'EVENT': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'LOC': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'PRODUCT': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'QUANTITY': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'GPE': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'MONEY': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'ORDINAL': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'Years of Experience': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'LAW': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'PERCENT': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'speed': 6394.043713812245}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Includes references for all of the above sections of the notebook"
      ],
      "metadata": {
        "id": "mfdKFI73e88o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ghosh, S., Majumder, S. and Santosh Kumar Das. (2023). Artificial Intelligence Techniques in Human Resource Management. Apple Academic Press (CRC Press). Available from https://doi.org/10.1201/9781003328346.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Pawan Budhwar et al. (2023). Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT. Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT, 33 (3). Available from https://doi.org/10.1111/1748-8583.12524.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Rahmani, D. and Kamberaj, H. (2021). Implementation and Usage of Artificial Intelligence Powered Chatbots in Human Resources Management Systems. International Conference on Social and Applied Sciences. May 2021. Available from https://www.researchgate.net/profile/Hiqmet-Kamberaj/publication/351345726_Implementation_and_Usage_of_Artificial_Intelligence_Powered_Chatbots_in_Human_Resources_Management_Systems/links/60926106299bf1ad8d78e1d1/Implementation-and-Usage-of-Artificial-Intelligence-Powered-Chatbots-in-Human-Resources-Management-Systems.pdf [Accessed 19 December 2023].\n",
        "\n",
        "<br/>\n",
        "\n",
        "Rosenbaum, E. (2019). IBM artificial intelligence can predict with 95% accuracy which workers are about to quit their jobs. CNBC. Available from https://www.cnbc.com/2019/04/03/ibm-ai-can-predict-with-95-percent-accuracy-which-employees-will-quit.html [Accessed 1 January 2024].\n",
        "\n",
        "<br/>\n",
        "\n",
        "Tambe, P., Cappelli, P. and Yakubovich, V. (2019). Artificial Intelligence in Human Resources Management: Challenges and a Path Forward. California Management Review, 61 (4), 15–42. Available from https://doi.org/10.1177/0008125619867910.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Vrontis, D. et al. (2021). Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review. The International Journal of Human Resource Management, 33 (6), 1–30. Available from https://doi.org/10.1080/09585192.2020.1871398."
      ],
      "metadata": {
        "id": "zNQyVKnZg6KD"
      }
    }
  ]
}