{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb52vw/e0ruDCNta/jcxUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RumethR/Resume-Ner/blob/main/Cw1_w1809911_RumethRandombage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Application Area Review\n",
        "\n",
        "**Selected Domain: Human resources and computing.**\n"
      ],
      "metadata": {
        "id": "Kg-JhFakYlkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The domain of human resources within an organisation entails a wide variety of operations and responsibilities ranging from employee compensation to organisational structure. This poses a great opportunity for Artificial Intelligent (AI) systems to contribute towards the Human Resource Management (HRM) tasks within an organisation. One of the most common areas where AI has been highly used is within the realm of recruitment and talent acquisition. A study by the ‘Oracle’ corporation has revealed that professionals in the human resource management industry believe that AI can boost productivity among other statistics (Rahmani and Kamberaj, 2021). Other sources believe that AI will enable new application procedures for candidates, increase applicant discovery rates and even reduce vacancy times as these systems can traverse through applications faster than trained professionals (Vrontis et al., 2021). AI powered applications in HRM are also able to process large amounts of data at high speeds, faster and more accurately than any trained professional, this allows organisations to gain insights that would otherwise be impossible with manual labour. The use of these systems are also more likely to be less biassed towards factors that do not affect performance such as age, gender or race (assuming that they are developed properly). Repetitive tasks within the domain of HRM are more likely to be successfully automated with AI powered applications according to *Vrontis et al. (2021)*.\n",
        "\n",
        "The overall effect of bias that is induced when training was studied by the influential work of *Tambe, Cappelli and Yakubovich (2019)* who observed that the data that these systems were built on had a major impact in how these systems performed. This underlying data itself has the opportunity to represent industry swings and unspoken ethics (i.e. the system might be trained on data that indicated towards hiring from a specific university), hence it is important that these systems work in tandem with trained professionals.This study also looks into how Machine Learning algorithms have fared against several different HR operations within organisations. AI powered HRM task management systems such as Quine, BenefitFocus and Jobvite are now widely available for any organisation of any scale to incorporate into their workflows (Tambe, Cappelli and Yakubovich, 2019).\n",
        "\n",
        "On the opposite side of the spectrum, employees now have access to tools such as IBM’s ‘Blue Match’ presents viable job openings for their specific skill set, similarly AI powered tools have helped IBM to identify possible flight risks and retain employees for much longer according to their former CEO Ginni Rometty (Rosenbaum, 2019). These HRM service vendors however do not provide any information about the type of AI techniques that are used internally within their products. It is also important to note that most AI powered solutions can typically be applied on a small portion of a HRM task, for example in the task of vetting candidates these tools can only provide a suggestion or score of desirability is most cases rather than performing the entire stack of recruiting tasks that go alongside the vetting process (Ghosh, Majumder and Santosh Kumar Das, 2023). The common theme among most studies that observe the usage of AI in HRM is that they are heavily reliant on the quality and the quantity of the data that they are being built upon.\n",
        "\n",
        "With the rise of open source Large Language Models (LLM) it is easier than ever for organisations to create a custom tailored algorithm that can perform trivial tasks within HRM such as answering frequently asked questions from employees through chatbots *Pawan Budhwar et al. (2023)* studied the use of popular generative AI tool ChatGPT in a HRM context and how it affects areas other than productivity such as employment relations, employee wellbeing and engagement. This study revealed that employees prefer tools such as ChatGPT in many scenarios that require understanding and perceiving information more than their human co-workers. However long term social, ethical and legal implications of using such technologies are difficult to quantify (Pawan Budhwar et al., 2023). Listed below are some other AI-based techniques used in the HRM context (Ghosh, Majumder and Santosh Kumar Das, 2023).\n",
        "\n",
        "*   Turnover Prediction with Artificial Neural Networks\n",
        "*   Candidate Search With Knowledge-Based Search Engines\n",
        "*   Sentiment Analysis on employee engagement and relations\n",
        "*   Résumé data acquisition using NLP\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PdkdKZArfPO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B: Comparision and evaluation of AI Techniques used in Human Resources"
      ],
      "metadata": {
        "id": "OWvidz0phH8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned before all HRM tasks in an organization cannot be supplemented or automated by one singular system. The scope of this specific section will cover the task of utilizing Machine Learning techniques for the purpose of recruiting, vetting and staffing employees using a CV/Résumé. This project will focus on using Named Entity Recognition (NER) on CVs to identify talent that best suites a given job posting. NER is classified as a Sequence Labelling Task within the domain of Natural Language Processing (NLP), there are many different ways of addressing these sequence labelling tasks listed below are three such approaches."
      ],
      "metadata": {
        "id": "itzimModhSzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using Conditional Random Fields (CRF)"
      ],
      "metadata": {
        "id": "m5YLiQ2f9-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Conditional Random Fields for NER tasks is a popular machine learning based approach which uses a probabilistic model. CRFs capture dependencies among neighbouring labels in a sequence and have the ability to consider the entire input sequence when making predictions. Feature functions are used to define relationships between input features (words) and output labels (entities). These relationships have to be manually defined by the trainer. CRFs calculate the probability distribution of an output sequence (labels) given an input sequence.Transition weights in Conditional Random Fields represent the likelihood of moving from one label to another in the sequence. During the training phase, the relationships between input features, their corresponding output labels (using data from the feature functions) and the effect of the transition weights are learned from the training data.\n",
        "\n",
        "For training a model that can perform NER using CRF text data with entities as labels are required. Necessary words in the training data sequence need to be accompanied by its corresponding entity label. Datasets for training NER models are widely available under different platforms such as Kaggle and the UCI Machine Learning Repository. One of the most common dataset and evaluation benchmark for NER is the CoNLL-2003 [dataset](https://www.clips.uantwerpen.be/conll2003/ner/). CRF based models tend to perform well as they have the ability to capture contextual information about the data. However performance is heavily dependent on the quality and relevance of engineered features, this is one of the major drawbacks of CRF based models. Feature engineering will mostly depend on the domain that the information is extracted from, and in the context of CVs people vary in the way they express information hence engineering features that capture necessary information will be quite a difficult task. Generally, CRF based models tend to be computationally efficient compared to their Deep Learning based counterparts who in turn will be better at capturing more intricate patterns within the data."
      ],
      "metadata": {
        "id": "YT0ru1sb-qMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using Neural Networks and pre-trained models"
      ],
      "metadata": {
        "id": "uQRcMfG2-IX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently there are many frameworks and pre-trained models that ease the task of NER. Similar to Tensorflow there are libraries that are built for performing NLP tasks. [SpaCy](https://spacy.io/usage/spacy-101) is one such Python package that offers pre-trained models for tasks such as NER. SpaCy’s default models use Convolutional Neural Networks (CNN) for the task of feature extraction and entity recognition. These models are trained on large labelled datasets hence they cover a wide range of contexts and features. By default the SpaCy package can identify entities within a  given text such as locations, dates, persons and organisations. These can be customised through a variety of API’s that are also included within the library. Similar to CRF and Transformer based models SpaCy’s CNNs can also understand contextual information regarding the input data, allowing the model to capture relationships between entities.\n",
        "\n",
        "Efficiency of a custom built CRF based model for NER depends heavily on the training data and the quality of the engineered features. Comparatively pre-trained models from SpaCy can offer similar accuracy scores and be more computationally efficient at  the same time. However by using a pre-built model the level of customization is limited. There are other libraries that have packaged in pre-trained models for NER such as [NLTK](https://www.nltk.org/) (Natural Language Toolkit) which can offer similar functionality to SpaCy, each library has their own strengths and weaknesses."
      ],
      "metadata": {
        "id": "5LRr3UGx_Jat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER using pre-trained Transformer based models (BERT)"
      ],
      "metadata": {
        "id": "E399fVqg-Mmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the rise of pre-trained Transformer based models such as BERT and GPT, the application of these models for NLP tasks has proven to yield competitive results. The Bi-directional approach considers text preceding from both the left and the right of a given word in a sentence. This helps the model to understand a much broader context (and longer text sequences) compared to the default pre-trained models offered by SpaCy which uses a CNN. This can be especially useful in the context of parsing data from CVs which can contain ambiguous and complex sentences. There are many more pre-trained Transformer based models which can have this characteristic, these are available through the [Hugging Face](https://huggingface.co/) community platform. BERT specifically is trained on a large corpus of text using 2 unsupervised learning techniques, masked language modelling and next sentence prediction.\n",
        "\n",
        "Unlike SpaCy, BERT is not an out-of-the-box solution for most NER tasks. This model needs to be fine-tuned in order to be used for NER purposes. This allows BERT to be domain specific and can in theory lead to better performance on very complex and domain specific NER tasks. The drawback of this level of customization is that a BERT model is more resource intensive and technically challenging to develop compared to SpaCy or NLTK. It also requires a labelled dataset for training before it can be used. Fine-tuning a BERT model requires large amounts of data to yield competitive results, which might not be publicly available in the case of CVs. In summary, BERT models can perform better than the other two approaches given that they are fine-tuned correctly, whilst SpaCy offers a better efficiency to performance ratio without requiring extensive training or customization. Although compared to training a CRF based model for NER form scratch customising a BERT model will yield better results, one drawback is it being more computationally intensive to train and use.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nf2NHgPdLZsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C: Implementation"
      ],
      "metadata": {
        "id": "bbGLUtHfjtY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using a text-based resume dataset available from Kaggle, the dataset will be accessed directly via this notebook using the Kaggle API. The credentials are hardcoded onto this notebook for ease-of-use during the viva. (Not Recommended)"
      ],
      "metadata": {
        "id": "HQY7HRr3tl_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"rumethrandombage\",\"key\":\"6c49cc180f0498fccf2dabb3d22f0236\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "import json\n",
        "\n",
        "!kaggle datasets download -d dataturks/resume-entities-for-ner\n",
        "!unzip resume-entities-for-ner.zip #Extracts to ./content\n",
        "\n",
        "dataset = [] # Each element contains a json object\n",
        "with open('/content/Entity Recognition in Resumes.json', 'r') as file:\n",
        "    for line in file:\n",
        "        dataset.append(json.loads(line))\n",
        "\n",
        "print(\"Number of elements in dataset list\", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I-uBUKswCVZ",
        "outputId": "b9e9b1ad-5705-4070-e7e3-45e7a52d1034"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "resume-entities-for-ner.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  resume-entities-for-ner.zip\n",
            "replace Entity Recognition in Resumes.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Number of elements in dataset list 220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 220 JSON objects. The structure of each JSON object is as follows:\n",
        "\n",
        "> \"content\" -> (contains all the data in the CV as a string)\n",
        "\n",
        "> \"annotations\" -> annotation label (\"Name\", \"Skills\", etc.) -> points (\"start and stop index where the label is contained\") - > text for label\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HpyO5Zi-CGmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy is packaged with a CNN based pre-built model that can perform NER for general entity types (Persons, Organizations, Locations, etc.)."
      ],
      "metadata": {
        "id": "XftpgsXqj1BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Access one string of text without any annotations from the datset and pass it into the pre-trained spacy model\n",
        "first_entry = dataset[0]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(first_entry[\"content\"])\n",
        "\n",
        "# Let's see how the pre-trained Model performed\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCYOyakvCGJN",
        "outputId": "79891347-21b7-482e-869e-9b268c16f7de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abhishek Jha\n",
            "Application PERSON\n",
            "Bengaluru GPE\n",
            "Karnataka - Email PERSON\n",
            "Karnataka GPE\n",
            "November 2017 DATE\n",
            "B.E in Information ORG\n",
            "B.v.b GPE\n",
            "Hubli ORG\n",
            "Karnataka GPE\n",
            "August 2013 to June 2017\n",
            "\n",
            "12th DATE\n",
            "Mathematics NORP\n",
            "April 2011 to March 2013 DATE\n",
            "10th ORDINAL\n",
            "Kendriya Vidyalaya PERSON\n",
            "April 2001 to March 2011 DATE\n",
            "Less than 1 year DATE\n",
            "Less than 1 year DATE\n",
            "Database Management ORG\n",
            "Less than 1 year DATE\n",
            "Database Management System ORG\n",
            "Less than 1 year DATE\n",
            "Java PERSON\n",
            "Less than 1 year DATE\n",
            "• Programming PRODUCT\n",
            "C++ PERSON\n",
            "Java\n",
            "• Oracle PERSON\n",
            "• Computer Networks ORG\n",
            "Linux GPE\n",
            "Windows GPE\n",
            "Non - Technical Skills\n",
            "\n",
            "• Honest ORG\n",
            "Calm\n",
            "• Team-Player PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The built-in NER model performs extremely poorly without any customisations. We can use transfer learning or we can create our own model using the NER pipeline provided by SpaCy, we will be creating a custom model for this project."
      ],
      "metadata": {
        "id": "HVhLRec3VUk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm # Used to show progress bars\n",
        "from spacy.util import filter_spans # To make sure there are no overlapping entities\n",
        "\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "labels = [\"Name\", \"College Name\", \"Degree\", \"Graduation Year\", \"Years of Experience\", \"Companies worked at\", \"Designation\", \"Skills\", \"Location\", \"Email Address\"]\n",
        "\n",
        "ner = nlp.add_pipe(\"ner\") #only using the ner component for the pipeline provided by SpaCy"
      ],
      "metadata": {
        "id": "oEso7kj8tZx7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# Add the pre-defined labels gathered from the dataset to the pipeline\n",
        "for label in labels:\n",
        "  ner.add_label(label)\n",
        "\n",
        "# Read the dataset and pre-process as needed for SpaCy\n",
        "print(\"Example before preprocessing data\")\n",
        "print(dataset[0])\n",
        "\n",
        "# Convert the data to spaCy's training data format\n",
        "formatted_data = []\n",
        "for entry in dataset:\n",
        "    text = entry['content'].replace(\"\\n\", \" \")\n",
        "    entities = []\n",
        "    for annot in entry['annotation']:\n",
        "        if len(annot['label']) != 1 :\n",
        "          continue\n",
        "          print(entry)\n",
        "        else:\n",
        "          label = annot['label'][0]\n",
        "          for point in annot['points']:\n",
        "            start = point['start']\n",
        "            end = point['end']\n",
        "\n",
        "            if start == end:\n",
        "              continue\n",
        "            else:\n",
        "              entities.append((start, end+1, label))\n",
        "\n",
        "    if len(entities) == 0:\n",
        "      continue\n",
        "    else:\n",
        "      formatted_data.append((text, {\"entities\": entities}))\n",
        "\n",
        "print(\"After formatting\")\n",
        "print(formatted_data[0])\n",
        "print(len(formatted_data))\n",
        "\n",
        "print(formatted_data[0][1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiUV0RRdnTHS",
        "outputId": "9a1f680f-5bb0-49f9-dc2c-e5590bae15b9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example before preprocessing data\n",
            "{'content': \"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\", 'annotation': [{'label': ['Skills'], 'points': [{'start': 1295, 'end': 1621, 'text': '\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player'}]}, {'label': ['Skills'], 'points': [{'start': 993, 'end': 1153, 'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]}, {'label': ['College Name'], 'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]}, {'label': ['College Name'], 'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]}, {'label': ['Graduation Year'], 'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]}, {'label': ['College Name'], 'points': [{'start': 771, 'end': 813, 'text': 'B.v.b college of engineering and technology'}]}, {'label': ['Designation'], 'points': [{'start': 727, 'end': 769, 'text': 'B.E in Information science and engineering\\n'}]}, {'label': ['Companies worked at'], 'points': [{'start': 407, 'end': 415, 'text': 'Accenture'}]}, {'label': ['Designation'], 'points': [{'start': 372, 'end': 404, 'text': 'Application Development Associate'}]}, {'label': ['Email Address'], 'points': [{'start': 95, 'end': 145, 'text': 'Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n'}]}, {'label': ['Location'], 'points': [{'start': 60, 'end': 68, 'text': 'Bengaluru'}]}, {'label': ['Companies worked at'], 'points': [{'start': 49, 'end': 57, 'text': 'Accenture'}]}, {'label': ['Designation'], 'points': [{'start': 13, 'end': 45, 'text': 'Application Development Associate'}]}, {'label': ['Name'], 'points': [{'start': 0, 'end': 11, 'text': 'Abhishek Jha'}]}], 'extras': None}\n",
            "After formatting\n",
            "(\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\", {'entities': [(1295, 1622, 'Skills'), (993, 1154, 'Skills'), (939, 957, 'College Name'), (883, 905, 'College Name'), (856, 861, 'Graduation Year'), (771, 814, 'College Name'), (727, 770, 'Designation'), (407, 416, 'Companies worked at'), (372, 405, 'Designation'), (95, 146, 'Email Address'), (60, 69, 'Location'), (49, 58, 'Companies worked at'), (13, 46, 'Designation'), (0, 12, 'Name')]})\n",
            "220\n",
            "{'entities': [(1295, 1622, 'Skills'), (993, 1154, 'Skills'), (939, 957, 'College Name'), (883, 905, 'College Name'), (856, 861, 'Graduation Year'), (771, 814, 'College Name'), (727, 770, 'Designation'), (407, 416, 'Companies worked at'), (372, 405, 'Designation'), (95, 146, 'Email Address'), (60, 69, 'Location'), (49, 58, 'Companies worked at'), (13, 46, 'Designation'), (0, 12, 'Name')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy requires training data to be in 'Doc' objects. The output from the pipeline will also result in a Doc object. Finally the file will be saved in the runtime with the .spacy format."
      ],
      "metadata": {
        "id": "3FbHKXMuxeh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the DocBin will store the example documents\n",
        "db = DocBin()\n",
        "for text, annotations in tqdm(formatted_data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in annotations['entities']:\n",
        "      span = doc.char_span(start, end, label=label)\n",
        "      if span is None:\n",
        "        print(start)\n",
        "        print(end)\n",
        "        print(label)\n",
        "\n",
        "      ents.append(span)\n",
        "    doc.ents = ents\n",
        "    db.add(doc)\n",
        "\n",
        "db.to_disk(\"./train.spacy\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "yc-R39CDyAX3",
        "outputId": "5c0138dc-4858-415f-f863-790f6617aed4"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/220 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)\n",
            "Kendriya Vidyalaya\n",
            "Woodbine modern school\n",
            "None\n",
            "B.v.b college of engineering and technology\n",
            "None\n",
            "Accenture\n",
            "Application Development Associate\n",
            "None\n",
            "Bengaluru\n",
            "Accenture\n",
            "Application Development Associate\n",
            "Abhishek Jha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-e658813046b0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0ments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.get_entity_info\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVQ2JBAjxeCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Includes references for all of the above sections of the notebook"
      ],
      "metadata": {
        "id": "mfdKFI73e88o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ghosh, S., Majumder, S. and Santosh Kumar Das. (2023). Artificial Intelligence Techniques in Human Resource Management. Apple Academic Press (CRC Press). Available from https://doi.org/10.1201/9781003328346.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Pawan Budhwar et al. (2023). Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT. Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT, 33 (3). Available from https://doi.org/10.1111/1748-8583.12524.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Rahmani, D. and Kamberaj, H. (2021). Implementation and Usage of Artificial Intelligence Powered Chatbots in Human Resources Management Systems. International Conference on Social and Applied Sciences. May 2021. Available from https://www.researchgate.net/profile/Hiqmet-Kamberaj/publication/351345726_Implementation_and_Usage_of_Artificial_Intelligence_Powered_Chatbots_in_Human_Resources_Management_Systems/links/60926106299bf1ad8d78e1d1/Implementation-and-Usage-of-Artificial-Intelligence-Powered-Chatbots-in-Human-Resources-Management-Systems.pdf [Accessed 19 December 2023].\n",
        "\n",
        "<br/>\n",
        "\n",
        "Rosenbaum, E. (2019). IBM artificial intelligence can predict with 95% accuracy which workers are about to quit their jobs. CNBC. Available from https://www.cnbc.com/2019/04/03/ibm-ai-can-predict-with-95-percent-accuracy-which-employees-will-quit.html [Accessed 1 January 2024].\n",
        "\n",
        "<br/>\n",
        "\n",
        "Tambe, P., Cappelli, P. and Yakubovich, V. (2019). Artificial Intelligence in Human Resources Management: Challenges and a Path Forward. California Management Review, 61 (4), 15–42. Available from https://doi.org/10.1177/0008125619867910.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Vrontis, D. et al. (2021). Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review. The International Journal of Human Resource Management, 33 (6), 1–30. Available from https://doi.org/10.1080/09585192.2020.1871398."
      ],
      "metadata": {
        "id": "zNQyVKnZg6KD"
      }
    }
  ]
}